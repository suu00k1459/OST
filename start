#!/usr/bin/env bash
# ============================================================================
# FLEAD Platform Startup Script (Linux/macOS/WSL)
# Single-broker Kafka mode - starts all Docker services and pipeline
# Usage: ./start [--fast] [--no-wait] [--safe]
# ============================================================================
set -e

# Go to repo root (directory of this script)
cd "$(dirname "$0")"

echo
echo "===================================================================="
echo "FEDERATED LEARNING PLATFORM - COMPLETE PIPELINE STARTUP (POSIX)"
echo "===================================================================="
echo

echo "Step 1: Checking Python..."
if ! command -v python3 >/dev/null 2>&1 && ! command -v python >/dev/null 2>&1; then
  echo "[ERROR] Python not found in PATH."
  exit 1
fi
echo "OK"
echo

echo "Step 2: Checking Docker..."
if ! command -v docker >/dev/null 2>&1; then
  echo "[ERROR] Docker not found in PATH or Docker daemon not running."
  exit 1
fi
echo "OK"
echo

echo "Checking Docker Compose..."
if docker compose version >/dev/null 2>&1; then
  DOCKER_COMPOSE=(docker compose)
elif command -v docker-compose >/dev/null 2>&1; then
  DOCKER_COMPOSE=(docker-compose)
else
  echo "[ERROR] Docker Compose (plugin or docker-compose) not found."
  exit 1
fi
echo "OK (using: ${DOCKER_COMPOSE[*]})"
echo

echo "Starting optional Jupyter UI (dev)..."
"${DOCKER_COMPOSE[@]}" up -d jupyter-dev || true
echo "  Jupyter UI (if started) at: http://localhost:8888"
echo

echo "Step 3: Running data preprocessor (Kaggle) in Docker..."

DEVICE_CSV="data/processed/device_0.csv"
CHUNKS_DIR="data/processed/chunks"

if [ ! -f "$DEVICE_CSV" ] && { [ ! -d "$CHUNKS_DIR" ] || [ -z "$(ls -A "$CHUNKS_DIR" 2>/dev/null)" ]; }; then
  echo "  No device CSVs or chunks found, running Docker data-preprocessor..."

  if [ ! -f "kaggle/kaggle.json" ]; then
    echo "[ERROR] kaggle/kaggle.json not found."
    echo "Please place your Kaggle API token file in:"
    echo "  kaggle/kaggle.json"
    exit 1
  fi

  if ! "${DOCKER_COMPOSE[@]}" run --rm data-preprocessor; then
    echo "  [WARNING] data-preprocessor container returned a non-zero exit code."

    if [ -f "$DEVICE_CSV" ] || { [ -d "$CHUNKS_DIR" ] && [ -n "$(ls -A "$CHUNKS_DIR" 2>/dev/null)" ]; }; then
      echo "  [INFO] Preprocessed data exists on disk (device CSVs or chunks). Treating preprocessing as successful."
    else
      echo "  [ERROR] No preprocessed data (device CSVs or chunks) were generated. Cannot continue."
      exit 1
    fi
  fi
else
  echo "  Preprocessed data already exists (device CSVs or chunks), skipping Docker preprocessing..."
fi
echo "OK"
echo

echo
echo "===================================================================="
echo "CLEANUP PHASE (POSIX)"
echo "===================================================================="
echo
"${DOCKER_COMPOSE[@]}" down || true
docker container rm -f zookeeper kafka timescaledb grafana kafka-ui \
  flink-jobmanager flink-taskmanager spark-master spark-worker-1 \
  kafka-broker-1 \
  timescaledb-collector federated-aggregator device-viewer \
  monitoring-dashboard grafana-init database-init 2>/dev/null || true

echo "Done"
echo

echo "===================================================================="
echo "DOCKER SERVICES STARTUP (POSIX)"
echo "===================================================================="
echo
"${DOCKER_COMPOSE[@]}" up -d
echo

echo "Waiting 30 seconds for services to come up (single-broker setup)..."
sleep 30
"${DOCKER_COMPOSE[@]}" ps
echo

echo "===================================================================="
echo "STARTING PIPELINE ORCHESTRATOR (POSIX)"
echo "===================================================================="
echo

# Handle optional startup flags
ORCH_FLAGS=""
for arg in "$@"; do
  case "$arg" in
    --fast|--no-wait|--safe)
      ORCH_FLAGS="$ORCH_FLAGS $arg"
      ;;
    *)
      ORCH_FLAGS="$ORCH_FLAGS $arg"
      ;;
  esac
done

if command -v python3 >/dev/null 2>&1; then
  python3 scripts/pipeline_orchestrator.py $ORCH_FLAGS
else
  python scripts/pipeline_orchestrator.py $ORCH_FLAGS
fi

echo
echo "===================================================================="
echo "PLATFORM STARTUP COMPLETE - ALL SERVICES IN DOCKER (POSIX)"
echo "===================================================================="
echo
echo "ACCESS POINTS:"
echo "  Live Monitoring:          http://localhost:5001"
echo "  Grafana Dashboard:        http://localhost:3001  (admin/admin)"
echo "  Kafka UI:                 http://localhost:8081"
echo "  Device Viewer Website:    http://localhost:8082"
echo "  Flink Dashboard:          http://localhost:8161"
echo "  Spark Master:             http://localhost:8086"
echo "  TimescaleDB:              localhost:5432"
echo
echo "LOGS:"
echo "  docker compose logs -f"
echo
