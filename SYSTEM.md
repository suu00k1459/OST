# FLEAD: Federated Learning for IoT Anomaly Detection with Multi-Broker Kafka

---

## SYSTEM OVERVIEW

FLEAD (Federated Learning for Edge Anomaly Detection) is a distributed machine learning system that trains 2,407 IoT devices locally across 4 independent Kafka brokers while coordinating globally to improve anomaly detection accuracy. The multi-broker architecture simulates realistic enterprise IoT deployments with independent data streams and fault tolerance.

### The Problem We're Solving:

-   Traditional ML: All data to central server, one model (Privacy risk, bandwidth waste)
-   FLEAD: Each device trains locally on independent broker, only model updates sent, central aggregation
-   Multi-Broker Benefit: 93% bandwidth reduction, privacy preserved, real-time detection, fault tolerance

### System Statistics:

-   Devices: 2,407 IoT sensors distributed across 4 brokers (600 per broker)
-   Data Points: 31,000+ messages/minute distributed across 4 independent Kafka brokers
-   Brokers: 4 independent Kafka brokers with replication factor 3
-   Model Versions: 70+ global models created through FedAvg
-   Average Accuracy: 72.7% (anomaly detection)

---

## ARCHITECTURE DIAGRAM

Multi-Broker Kafka Cluster:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                 FLEAD FEDERATED LEARNING WITH MULTI-BROKER KAFKA            │
└─────────────────────────────────────────────────────────────────────────────┘

                     ┌──────────────────────────────┐
                     │    2407 IoT Devices          │
                     │  (2400 preprocessed CSV)     │
                     └──────────────┬───────────────┘
                                    │
              ┌─────────────────────┴─────────────────────┐
              │                                           │
              ▼                                           ▼
    ┌──────────────────────────────┐    ┌──────────────────────────────┐
    │     KAFKA MULTI-BROKER       │    │   DEVICE DISTRIBUTION        │
    │     (4 Independent Brokers)  │    │   (devices modulo 4)         │
    │                              │    │                              │
    │ Broker 1: Port 29092/9092    │    │ Broker 1: device_0-599      │
    │ Broker 2: Port 29093/9093    │◄───┤ Broker 2: device_600-1199   │
    │ Broker 3: Port 29094/9094    │    │ Broker 3: device_1200-1799  │
    │ Broker 4: Port 29095/9095    │    │ Broker 4: device_1800-2399  │
    │                              │    │                              │
    │ KRaft Mode (no Zookeeper)    │    │ 600 devices per broker       │
    │ Replication Factor: 3        │    │ Replicated across 3 brokers  │
    │ Bootstrap Servers:           │    │                              │
    │ broker-1:29092,              │    │                              │
    │ broker-2:29093,              │    │                              │
    │ broker-3:29094,              │    │                              │
    │ broker-4:29095               │    │                              │
    └──────────┬───────────────────┘    └──────────────────────────────┘
               │
   Topics: edge-iiot-stream (replicated across all brokers)
           anomalies (Flink detections)
           local-model-updates (device models)
           global-model-updates (FedAvg results)
               │
    ┌──────────┴──────────┬──────────────┬──────────────┐
    │                     │              │              │
    ▼                     ▼              ▼              ▼
┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│ FLINK ALL    │  │ FLINK ALL    │  │ SPARK ALL    │  │ SPARK ALL    │
│ BROKERS      │  │ BROKERS      │  │ BROKERS      │  │ BROKERS      │
│ (JobManager) │  │ (TaskManager)│  │ (Master)     │  │ (Worker)     │
│              │  │              │  │              │  │              │
│ Consumes:    │  │ Processes:   │  │ Reads:       │  │ Executes:    │
│ All 4        │  │ Streaming    │  │ All 4        │  │ Batch        │
│ brokers      │  │ data from    │  │ brokers      │  │ analytics    │
│              │  │ all brokers  │  │              │  │ on all data  │
│ Real-time:   │  │              │  │ Batch:       │  │              │
│ - Z-score    │  │ Per-device   │  │ Hourly       │  │ Distributed  │
│   anomaly    │  │ local SGD    │  │ - Trends     │  │ computation  │
│   detection  │  │ training     │  │ - Metrics    │  │ on 2400      │
│ - Per-broker │  │              │  │ - Analysis   │  │ devices      │
│   stream     │  │ Output:      │  │              │  │              │
│              │  │ local-model- │  │ Stream:      │  │ Results to   │
│ Output to:   │  │ updates      │  │ - Evaluates  │  │ TimescaleDB  │
│ anomalies    │  │              │  │   global     │  │              │
│ topic        │  │              │  │   models     │  │              │
│              │  │              │  │ - Calculates │  │              │
│              │  │              │  │   accuracy   │  │              │
└──────┬───────┘  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘
       │                 │                 │                 │
       │                 ▼                 │                 │
       │          local-model-updates      │                 │
       │          (device accuracy)        │                 │
       │                 │                 │                 │
       │                 ▼                 │                 │
       │  ┌──────────────────────────────┐ │                 │
       │  │ FEDERATED AGGREGATION        │ │                 │
       │  │ (FedAvg from all brokers)    │ │                 │
       │  │                              │ │                 │
       │  │ Consumes: local-model-       │ │                 │
       │  │           updates from ALL   │ │                 │
       │  │           devices across     │ │                 │
       │  │           4 brokers          │ │                 │
       │  │                              │ │                 │
       │  │ Buffer: 20 device updates    │ │                 │
       │  │ Aggregate: Weighted average  │ │                 │
       │  │           (by samples)       │ │                 │
       │  │ Output: global-model-updates │ │                 │
       │  │ (v1, v2, v3... v70)          │ │                 │
       │  └──────────┬───────────────────┘ │                 │
       │             │                     │                 │
       │             ▼                     │                 │
       │    global-model-updates           │                 │
       │                                   │                 │
       └───────────┬───────────────────────┴─────────────────┘
                   │                           │
                   ▼                           ▼
    ┌─────────────────────────────────────────────────────────┐
    │     TIMESCALEDB (Time-Series Database)                  │
    │     Persistent Storage for All Components               │
    │                                                         │
    │  Tables:                                                │
    │  ├─ local_models (11,247 from all 4 brokers)            │
    │  │  └─ device_id, model_version, accuracy, samples      │
    │  │                                                      │
    │  ├─ federated_models (70+ global versions)              │
    │  │  └─ global_version, aggregation_round, num_devices   │
    │  │                                                      │
    │  ├─ model_evaluations (Spark evaluation results)        │
    │  │  └─ global_version, prediction_result, accuracy      │
    │  │                                                      │
    │  └─ anomalies (Flink detections from all brokers)       │
    │     └─ device_id, broker_id, timestamp, z_score         │
    └──────────────────────┬──────────────────────────────────┘
                           │
                           ▼
                ┌──────────────────────┐
                │      GRAFANA         │
                │     Dashboards       │
                │ (SQL queries to DB)  │
                │                      │
                │ Shows:               │
                │ - 70 global versions │
                │ - 11K+ local models  │
                │ - 72.7% accuracy     │
                │ - Trends across 4    │
                │   brokers            │
                │ - Device rankings    │
                │ - Real-time metrics  │
                └──────────────────────┘
```

### Key Multi-Broker Features:

1. Device Distribution: Each device permanently assigned to one broker (device_id modulo 4)
2. Data Streams: 4 independent, parallel data streams through 4 brokers
3. Fault Tolerance: Replication factor 3 means data survives broker failures
4. Load Balancing: 600 devices per broker (equal distribution)
5. Scalability: Easy to add more brokers for more devices
6. Consumer Access: All consumers (Flink, Spark, Aggregator) connect to all 4 brokers
  

**Training rate :**
- Shows: "How many new models per minute?"
- Example: "23 models/minute"
- Tells you if system is working fast or slow


---

## COMPONENT DEEP DIVE

### KAFKA: MULTI-BROKER MESSAGE CLUSTER

**Role:** Central nervous system with 4 independent brokers - routes data between all components

**Architecture: 4-Broker KRaft Cluster**

- Broker 1 (Port 29092): devices_0-599, Controller quorum member
- Broker 2 (Port 29093): devices_600-1199, Controller quorum member
- Broker 3 (Port 29094): devices_1200-1799, Controller quorum member
- Broker 4 (Port 29095): devices_1800-2399, Broker only

**What it does:**

- Receives raw IoT sensor data from 2,407 devices (31,000+ msgs/minute distributed across 4 brokers)
- Routes messages to correct broker based on device_id
- Buffers data in replicated topics so components can process independently
- Guarantees no data loss with replication factor 3 (3 brokers hold each message)

**Multi-Broker Benefits:**

1. Fault Tolerance: If one broker fails, 2 backups available
2. Load Balancing: 600 devices per broker (equal distribution)
3. Scalability: Easy to add more brokers for growth
4. Realistic Simulation: Matches enterprise IoT deployments
5. Independent Streams: Each broker processes subset independently





**Topics we use:**

| Topic                  | Producer              | Consumers             | Purpose                            | Replication |
| ---------------------- | --------------------- | --------------------- | ---------------------------------- | ----------- |
| `edge-iiot-stream`     | Kafka Producer        | Flink, Spark          | Raw sensor data from all devices   | Factor 3    |
| `anomalies`            | Flink (all brokers)   | Storage, Monitoring   | Detected anomalies (Z-score > 2.5) | Factor 3    |
| `local-model-updates`  | Flink (all brokers)   | Federated Aggregation | Local model accuracies per device  | Factor 3    |
| `global-model-updates` | Federated Aggregation | Spark                 | New global model versions          | Factor 3    |

**Deployment:** Docker containers - 4 Kafka brokers in KRaft mode (no Zookeeper needed)

---

### FLINK: REAL-TIME TRAINING

**Role:** Trains 2,407 local models in parallel on streaming data from all 4 brokers

**What it does:**

```

For each IoT device (independently across all 4 brokers):

1. Collects streaming measurements from assigned broker
2. Calculates statistics (mean, std)
3. Detects anomalies using Z-score
4. Every 50 rows OR 60 seconds:
   → Trains local ML model using Stochastic Gradient Descent (SGD)
   → Calculates accuracy
   → Publishes to Kafka via local-model-updates topic

````

**Multi-Broker Processing:**

- JobManager: Connects to all 4 brokers' bootstrap servers
- TaskManager: Processes stream from all 4 brokers in parallel
- Each device maintained independently (v1, v2, v3... per device)
- Models trained at different rates (faster devices create more versions)

**Health:** Flink components depend on all 4 brokers being healthy before starting

```python
z_score = (value - mean) / std_dev  # 3 math operations
if z_score > 2.5:
    send_alert()
````

**The SGD Training Algorithm:**

```python
# For each training batch:
for measurement in batch:
    # 1. PREDICT: What does current model think?
    prediction = sigmoid(weights · features + bias)

    # 2. MEASURE ERROR: How wrong were we?
    error = prediction - actual_label

    # 3. CALCULATE GRADIENT: Which direction to fix?
    gradient = error × features

    # 4. UPDATE WEIGHTS: Step in the right direction
    weights = weights - (learning_rate × gradient)

    # 5. REPEAT: Next measurement in batch
```

**Key Parameters:**

-   **Learning Rate:** 0.001 (small steps, stable learning)
-   **Batch Size:** 50 measurements
-   **Features:** [mean, std_dev, z_score]
-   **Model Type:** Logistic Regression (binary classifier)

**Example Output:**

```
Device 127: v3 - Accuracy: 76.43%, Loss: 0.4521, Updates: 180
Device 256: v2 - Accuracy: 62.89%, Loss: 0.6234, Updates: 120
Device 1008: v4 - Accuracy: 81.23%, Loss: 0.3891, Updates: 240
```

**Different versions per device:**

-   Device 127 is at model v3 (trained 3 times)
-   Device 256 is at model v2 (trained 2 times)
-   Device 1008 is at model v4 (trained 4 times)
-   ✓ **This is NORMAL!** Devices train at different speeds based on data

**Why SGD?**

-   Works on streaming data (doesn't need all data upfront)
-   Updates continuously as new data arrives
-   Computationally light for edge devices
-   Converges quickly with small learning rate

**Deployment:** Apache Flink 1.18 running in Docker container, connected to Kafka at `kafka:29092`

---

### FEDERATED AGGREGATION: FEDAVG

**Role:** Creates global model by combining local models from devices

**What it does:**

```
Step 1: Collect local model updates
        From Kafka topic: local-model-updates

Step 2: Buffer updates (wait for ~20 updates from different devices)

Step 3: Calculate global accuracy (weighted average)
        Global Accuracy = Σ(Device_Accuracy × Samples_Processed) / Σ(Samples_Processed)

Step 4: Create new global model version
        - Increment version counter (v1 → v2 → v3...)
        - Store in database
        - Publish new version to Kafka

Step 5: Reset buffer, repeat
```

**The FedAvg Algorithm:**

```
Global Model v1: weights = [0.5, 0.3, 0.2]

Local Updates from devices:
  Device 1: weights = [0.45, 0.35, 0.25], samples = 500
  Device 5: weights = [0.55, 0.25, 0.20], samples = 400
  Device 8: weights = [0.48, 0.32, 0.23], samples = 300

Federated Averaging:
  w_global = (w1×500 + w5×400 + w8×300) / (500 + 400 + 300)
           = ([0.45×500 + 0.55×400 + 0.48×300] / 1200, ...)
           = [0.494, 0.323, 0.223]

Global Model v2: weights = [0.494, 0.323, 0.223]  ← NEW VERSION
```

**Global vs Local Versions:**

| Type       | Example       | Scope         | Purpose                                     |
| ---------- | ------------- | ------------- | ------------------------------------------- |
| **LOCAL**  | Device 127 v3 | Single device | That device's personal trained model        |
| **GLOBAL** | v70           | All devices   | Aggregated knowledge from all 2,407 devices |

**Current Status:**

-   70 global models created (v1 through v70)
-   11,247 local models trained total
-   Each global version incorporates models from ~20 devices

**Example Flow:**

```
Minute 1: Device 42 trains, publishes v1 → Buffer size = 1
Minute 2: Device 128 trains, publishes v1 → Buffer size = 2
...
Minute 20: Device 2010 trains, publishes v1 → Buffer size = 20

         ✓ Buffer full! Trigger aggregation

         → Calculate global v71 from all 20 local updates
         → Publish to: global-model-updates topic
         → Clear buffer
         → Start collecting for v72
```

**Why FedAvg?**

-   **Privacy:** Only model weights sent to server, not raw data
-   **Efficient:** Reduces communication by 99% (vs sending all data)
-   **Decentralized:** Each device trains independently
-   **Proven:** Used by Google, Apple, Meta in production

**Deployment:** Python service using Kafka consumer/producer, runs continuously

---

### SPARK: ANALYTICS & EVALUATION (Multi-Broker)

**Role:** Evaluates global models on real data from all 4 brokers

**Multi-Broker Design:**

-   Master: Connects to all 4 broker bootstrap servers
-   Workers: Process data from all brokers in parallel
-   Batch Jobs: Read complete dataset across all 4 brokers
-   Stream Evaluation: Test models on data from all devices

**Spark Batch Analytics (hourly):**

```
Every 1 hour:
  1. Read all streaming data from all 4 Kafka brokers
  2. Analyze patterns and trends across 2400 devices
  3. Calculate statistics (avg, std, anomaly rate per broker)
  4. Store in TimescaleDB for reporting
```

**Spark Stream Evaluation (continuous):**

```
For each global model version (v1, v2, v3... v70):
  1. Download model weights from TimescaleDB
  2. Stream new IoT data from all 4 brokers through model
  3. Get predictions: "Is this anomalous?"
  4. Compare to actual labels
  5. Calculate REAL accuracy: ✓ correct / ✗ incorrect (weighted by broker)
  6. Store results in model_evaluations table
```

**Health:** Spark components depend on all 4 brokers being healthy before starting

**Example Evaluation:**

```
Testing Global Model v70 on 1000 new samples:

Sample 1:
  Actual: "normal" (label=0)
  Prediction: "normal" (0.23 prob) ✓ CORRECT

Sample 2:
  Actual: "anomaly" (label=1)
  Prediction: "normal" (0.34 prob) ✗ WRONG

Sample 3:
  Actual: "normal" (label=0)
  Prediction: "anomaly" (0.78 prob) ✗ WRONG

...continuing for 1000 samples...

Result:
  ✓ Correct: 727 samples
  ✗ Wrong: 273 samples
  Real Accuracy: 727/1000 = 72.7%
```

**Key Metrics Calculated:**

-   **True Positives (TP):** Predicted anomaly, actually anomaly ✓
-   **False Positives (FP):** Predicted anomaly, actually normal ✗
-   **True Negatives (TN):** Predicted normal, actually normal ✓
-   **False Negatives (FN):** Predicted normal, actually anomaly ✗

**Derived Metrics:**

-   **Accuracy:** (TP + TN) / (TP + TN + FP + FN)
-   **Precision:** TP / (TP + FP)
-   **Recall:** TP / (TP + FN)
-   **F1-Score:** 2 × (Precision × Recall) / (Precision + Recall)

**Why Spark?**

-   **Parallel Processing:** Evaluates multiple models simultaneously
-   **Big Data:** Handles gigabytes of data efficiently
-   **Batch + Stream:** Can do historical and real-time analysis
-   **Integration:** Easy connection to Kafka and databases

**Deployment:** Apache Spark 3.5.0, Master + Worker nodes in Docker

---

### TIMESCALEDB: STORAGE

**Role:** Time-series database that stores all models and results

**What is TimescaleDB?**

-   PostgreSQL extension optimized for time-series data
-   Automatically partitions data by time (hypertables)
-   Super fast queries on time-windowed data

**Tables in our system:**

#### 1. `local_models` (11,247 records)

```sql
CREATE TABLE local_models (
    id SERIAL PRIMARY KEY,
    device_id TEXT NOT NULL,           -- "device_127"
    model_version INT NOT NULL,         -- 1, 2, 3, 4...
    global_version INT NOT NULL,        -- Which global version was active
    accuracy FLOAT NOT NULL,            -- 0.62, 0.78, 0.81
    samples_processed INT NOT NULL,     -- 500, 400, 600
    created_at TIMESTAMPTZ NOT NULL     -- 2025-11-07 15:22:31
);
```

**Example Row:**

```
device_127 | v3 | global_v70 | 0.7643 | 500 | 2025-11-07 15:22:31
```

#### 2. `federated_models` (70 records)

```sql
CREATE TABLE federated_models (
    id SERIAL PRIMARY KEY,
    global_version INT NOT NULL,        -- v1, v2, v3... v70
    aggregation_round INT NOT NULL,     -- Which round created this
    num_devices INT NOT NULL,           -- How many devices contributed
    accuracy FLOAT NOT NULL,            -- 0.727 (weighted average)
    created_at TIMESTAMPTZ NOT NULL
);
```

**Example Row:**

```
global_v70 | round_70 | 20_devices | 0.727 | 2025-11-07 15:22:24
```

#### 3. `model_evaluations` (Spark results)

```sql
CREATE TABLE model_evaluations (
    id SERIAL PRIMARY KEY,
    model_version INT NOT NULL,         -- Which model was tested
    device_id TEXT NOT NULL,            -- Which device evaluated it
    prediction FLOAT NOT NULL,          -- 0.78 (model's probability)
    actual_result INT NOT NULL,         -- 0 or 1 (ground truth)
    is_correct BOOLEAN NOT NULL,        -- true or false
    confidence FLOAT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL
);
```

**Example Row:**

```
global_v70 | device_0 | 0.78 | 1 | true | 0.95 | 2025-11-07 15:20:00
```

#### 4. `anomalies` (Flink detections)

```sql
CREATE TABLE anomalies (
    device_id TEXT NOT NULL,
    value FLOAT NOT NULL,               -- 45.2
    z_score FLOAT NOT NULL,             -- 3.2 (how extreme?)
    severity TEXT NOT NULL,             -- "warning" or "critical"
    timestamp TIMESTAMPTZ NOT NULL
);
```

**Query Examples:**

```sql
-- What's the average accuracy per device?
SELECT device_id, AVG(accuracy) as avg_accuracy, COUNT(*) as models_trained
FROM local_models
GROUP BY device_id
ORDER BY avg_accuracy DESC
LIMIT 10;

-- How many models per global version?
SELECT global_version, COUNT(*) as num_local_models
FROM local_models
GROUP BY global_version
ORDER BY global_version DESC;

-- Latest 5 global models
SELECT global_version, accuracy, num_devices, created_at
FROM federated_models
ORDER BY created_at DESC
LIMIT 5;
```

**Why TimescaleDB?**

-   **Time-series optimized:** Hypertables compress time data 90%+
-   **SQL power:** Full SQL queries, not limited like NoSQL
-   **Performance:** Queries 10-100x faster than regular PostgreSQL
-   **Retention:** Easy to delete old data with compression policies

**Deployment:** Docker container, stores data in persistent volume

---

### GRAFANA: VISUALIZATION

**Role:** Beautiful real-time dashboards for monitoring and analysis

**Grafana Dashboard Panels (8 total):**

#### 1. **Total Local Models** (1,403)

-   Shows count of all device models trained
-   Updates every 2 seconds

#### 2. **Global Models Created** (70)

-   Count of global versions created through FedAvg
-   Each represents a complete aggregation round

#### 3. **Active Devices (Last Hour)** (970)

-   How many devices have sent data in past hour
-   Indicates system health

#### 4. **Latest Global Accuracy** (72.7%)

-   Most recent global model's weighted accuracy
-   From federated_models table

#### 5. **Model Training Rate (Per Minute)**

-   New models created per minute
-   Shows training speed

#### 6. **Global Model Accuracy Trend**

-   Line chart over time
-   Shows how accuracy evolves as we create more versions

#### 7. **Top Devices by Training Count**

```
device_127   → 560 models trained
device_256   → 515 models trained
device_1008  → 540 models trained
```

#### 8. **Recent Federated Models Table**

| Version | Round | Devices | Accuracy | Created At |
| ------- | ----- | ------- | -------- | ---------- |
| 70      | 70    | 20      | 72.7%    | 15:22:31   |
| 69      | 69    | 19      | 73.9%    | 15:22:24   |
| 68      | 68    | 20      | 75.6%    | 15:22:09   |

**How Grafana Connects:**

```
Grafana → TimescaleDB (SQL queries)
        → Reads: local_models, federated_models, model_evaluations
        → Plots data automatically
        → Refreshes every 30 seconds
```

**Why Grafana?**

-   **Real-time:** Updates every 30 seconds automatically
-   **SQL Native:** Query TimescaleDB directly with SQL
-   **Beautiful:** Professional-looking dashboards
-   **Interactive:** Click, zoom, filter data on dashboard

**Deployment:** Docker container at `http://localhost:3001` (admin/admin)

---

## DATA FLOW PIPELINE

### Complete End-to-End Journey of One Data Point:

```
TIME: 2025-11-07 15:22:31.000

STEP 1: GENERATION (IoT Device)
─────────────────────────────────
Device 127 measures temperature: 42.5°C
Sends: {
  "device_id": "device_127",
  "data": 42.5,
  "timestamp": "2025-11-07T15:22:31.000Z"
}


STEP 2: INGESTION (Kafka)
──────────────────────────
Message lands in: edge-iiot-stream topic
Kafka stores it with timestamp
Partition: device_127 (ensures ordering per device)
Offset: 45321
Replicas: 1 broker (can increase for HA)


STEP 3: REAL-TIME PROCESSING (Flink - Parallel)
────────────────────────────────────────────────
Flink worker receives message
Updates device_127's statistics:
  Mean: 42.1°C (running average)
  Std Dev: 1.8°C (volatility)

Calculates Z-score: (42.5 - 42.1) / 1.8 = 0.22
Compare to threshold: 0.22 < 2.5 → NORMAL (not anomalous)

Check if model training needed:
  Samples since last train: 47/50
  Time since last train: 45/60 seconds
  → Not yet, keep collecting

Send to Kafka: anomalies topic (EMPTY - no anomaly detected)


STEP 4: BATCH TRAINING (Flink - After 50 samples or 60 sec)
──────────────────────────────────────────────────────────
[Time: 15:23:31 - 60 seconds later]

Flink has collected 50 measurements for device_127
Prepares training data:
  X_train = [
    [42.1, 1.8, 0.22],   # Features: [mean, std, z_score]
    [41.9, 1.7, -0.12],
    [42.3, 1.9, 0.11],
    ... (47 more)
  ]
  y_train = [0, 0, 0, 1, 0, ...]  # Labels: 1=anomaly, 0=normal

Trains SGD model:
  Iteration 1: Update weights based on sample 1
  Iteration 2: Update weights based on sample 2
  ...
  Iteration 50: Final weight update

  Loss after training: 0.4521
  Accuracy on training data: 76.43%

Increments version: device_127 v2 → v3

Publishes to Kafka: local-model-updates
{
  "device_id": "device_127",
  "model_version": 3,
  "accuracy": 0.7643,
  "loss": 0.4521,
  "samples_processed": 50,
  "timestamp": "2025-11-07T15:23:31.000Z"
}


STEP 5: FEDERATED AGGREGATION (Flink Runner - Every 20 Updates)
───────────────────────────────────────────────────────────────
Federated Aggregation service reads from: local-model-updates

Buffers updates:
  Device 127 v3 (0.7643 accuracy, 50 samples)
  Device 256 v2 (0.6289 accuracy, 40 samples)
  Device 1008 v4 (0.8123 accuracy, 60 samples)
  ... (17 more devices)

[After 20 devices → Trigger aggregation]

Calculate global accuracy:
  weighted_acc = (0.7643×50 + 0.6289×40 + 0.8123×60 + ...) / (50+40+60+...)
               = 0.727 (72.7%)

Create new global model v71:
  weights = average of all 20 device weights
  version = 71
  accuracy = 0.727
  num_devices = 20
  timestamp = 2025-11-07T15:24:00

Save to database: INSERT INTO federated_models (...)

Publish to Kafka: global-model-updates
{
  "version": 71,
  "aggregation_round": 71,
  "global_accuracy": 0.727,
  "num_devices": 20,
  "timestamp": "2025-11-07T15:24:00Z"
}


STEP 6: STORAGE (TimescaleDB)
──────────────────────────────
Local model stored:
  INSERT INTO local_models
    (device_id, model_version, global_version, accuracy, samples_processed, created_at)
  VALUES ('device_127', 3, 71, 0.7643, 50, NOW())

Global model stored:
  INSERT INTO federated_models
    (global_version, aggregation_round, num_devices, accuracy, created_at)
  VALUES (71, 71, 20, 0.727, NOW())

Hypertable partitions by time (hourly):
  local_models_2025_11_07_15
  federated_models_2025_11_07_15


STEP 7: BATCH EVALUATION (Spark - Every hour)
───────────────────────────────────────────────
Spark reads global_v71 from federated_models
Downloads weights from storage
Streams IoT data through model

For each test sample:
  Input: [mean=42.1, std=1.8, z_score=0.22]
  Sigmoid output: 0.35
  Prediction: NORMAL (< 0.5)
  Actual label: 0 (NORMAL)
  Result: CORRECT ✓

Repeat for 1000 samples...
Collect: 727 correct, 273 wrong
Real Accuracy: 72.7%

Store in database:
  INSERT INTO model_evaluations
    (model_version, prediction, actual_result, is_correct, ...)


STEP 8: VISUALIZATION (Grafana)
────────────────────────────────
Grafana dashboard queries TimescaleDB every 30 seconds:

SELECT accuracy FROM federated_models ORDER BY created_at DESC LIMIT 1
→ Returns: 0.727

SELECT COUNT(*) FROM local_models WHERE created_at > NOW() - INTERVAL '1 hour'
→ Returns: 1403 (total local models)

SELECT * FROM federated_models ORDER BY created_at DESC LIMIT 10
→ Displays table of last 10 global versions

SELECT AVG(accuracy) FROM local_models
GROUP BY device_id
ORDER BY AVG(accuracy) DESC LIMIT 10
→ Shows top 10 devices by accuracy

Dashboard updates:
  ✓ Latest Global Accuracy: 72.7%
  ✓ Total Local Models: 1,403
  ✓ Global Models: 71
  ✓ Training Rate: 23.4 models/minute
  ✓ Accuracy Trend: Line chart shows 72% → 73% → 74% → ... → 72.7%

USER VIEWS: Beautiful real-time dashboard
```

---

## HOW COMPONENTS LINK TOGETHER

### Data Flow Architecture:

```
┌─────────────────────────────────────────────────────────────────┐
│                     SYSTEM INTEGRATION POINTS                    │
└─────────────────────────────────────────────────────────────────┘

SYNCHRONIZATION:
───────────────
All components use ISO 8601 timestamps (UTC) for synchronization:
  Format: 2025-11-07T15:22:31.000Z
  Ensures: No timing conflicts, reproducible results

KAFKA (Hub):
─────────────
Topic: edge-iiot-stream
  ├─ Source: IoT Devices (2,407)
  ├─ Consumers: Flink (real-time), Spark (batch)
  └─ Purpose: Raw data distribution

Topic: anomalies
  ├─ Source: Flink (anomaly detection)
  ├─ Consumers: TimescaleDB (storage), Monitoring
  └─ Purpose: Alert on detected anomalies

Topic: local-model-updates
  ├─ Source: Flink (after training)
  ├─ Consumers: Federated Aggregation, TimescaleDB
  └─ Purpose: Device accuracy reporting

Topic: global-model-updates
  ├─ Source: Federated Aggregation (FedAvg)
  ├─ Consumers: Flink, TimescaleDB
  └─ Purpose: Broadcast new global versions


FLINK → KAFKA → AGGREGATION:
─────────────────────────────
Flink trains device models
            ↓
Publishes accuracy to: local-model-updates
            ↓
Federated Aggregation consumes
            ↓
Calculates weighted average
            ↓
Publishes new global model to: global-model-updates
            ↓
Stored in TimescaleDB


SPARK → TIMESCALEDB:
────────────────────
Spark reads global model from database
            ↓
Evaluates on real test data
            ↓
Calculates accuracy metrics
            ↓
Stores results in model_evaluations table
            ↓
Grafana queries results


TIMESCALEDB ← ALL COMPONENTS:
─────────────────────────────
Flink → Stores local_models
Aggregation → Stores federated_models
Spark → Stores model_evaluations
            ↓
TimescaleDB organizes by time
            ↓
Grafana queries with SQL


GRAFANA QUERIES:
────────────────
Dashboard.Panel_1 (Latest Accuracy):
  SELECT accuracy FROM federated_models
  ORDER BY created_at DESC LIMIT 1
  ↓ Returns: 72.7%

Dashboard.Panel_2 (Model Count):
  SELECT COUNT(*) FROM local_models
  ↓ Returns: 1,403

Dashboard.Panel_3 (Trend Chart):
  SELECT created_at, accuracy FROM federated_models
  ORDER BY created_at DESC LIMIT 100
  ↓ Returns: Time series for line chart

Dashboard.Panel_4 (Top Devices):
  SELECT device_id, AVG(accuracy), COUNT(*) as models_trained
  FROM local_models
  GROUP BY device_id
  ORDER BY models_trained DESC LIMIT 10
  ↓ Returns: Device ranking table
```

### Dependency Order (What must start first):

```
1. TIMESCALEDB (Database must exist first)
   └─ Creates tables for storing everything

2. KAFKA (Message hub must be ready)
   └─ Receives data from all sources

3. FLINK (Processes incoming data)
   └─ Consumes from: edge-iiot-stream
   └─ Produces to: anomalies, local-model-updates

4. FEDERATED AGGREGATION (Needs Flink models)
   └─ Consumes from: local-model-updates
   └─ Produces to: global-model-updates

5. SPARK (Evaluates completed models)
   └─ Reads from: federated_models (TimescaleDB)
   └─ Writes to: model_evaluations (TimescaleDB)

6. GRAFANA (Visualizes results)
   └─ Connects to: TimescaleDB
   └─ Queries: All tables
   └─ Displays: Real-time dashboards
```

---

## KEY ALGORITHMS

### 1. Z-Score Anomaly Detection (Flink)

**What it does:** Detects unusual measurements compared to device's history

**Formula:**

```
Z-score = (current_value - mean) / standard_deviation
```

**Example:**

```
Device 127 history: [42.1, 42.3, 41.9, 42.0, 42.2]
Mean: 42.1°C
Std Dev: 0.15°C

New measurement: 45.0°C
Z-score = (45.0 - 42.1) / 0.15 = 19.3

Threshold: 2.5
19.3 > 2.5 → ANOMALY DETECTED! (Critical)
```

**Interpretation:**

-   Z-score = 1.0: 1 std dev away (normal)
-   Z-score = 2.5: 2.5 std dev away (anomalous)
-   Z-score = 3.0: Very unusual (critical)

---

### 2. Stochastic Gradient Descent (Flink)

**What it does:** Learns to predict anomalies from historical data

**Algorithm:**

```
Initialize: weights = [0.0, 0.0, 0.0], bias = 0.0

For each training epoch:
  For each sample in batch:
    1. Prediction = sigmoid(weights · features + bias)
    2. Loss = -label × log(prediction) - (1-label) × log(1-prediction)
    3. Gradient = (prediction - label) × features
    4. weights = weights - learning_rate × gradient
    5. bias = bias - learning_rate × (prediction - label)

Result: Model that predicts P(anomaly | features)
```

**Why it works:**

-   Learns from streaming data (one sample at a time)
-   Adapts to device-specific patterns
-   Lightweight (only 4 parameters: 3 weights + 1 bias)
-   Interpretable (can see which features matter)

---

### 3. Federated Averaging - FedAvg (Aggregation Service)

**What it does:** Combines models from multiple devices into one global model

**Algorithm:**

```
Global Model v1 → weights_global = [0.5, 0.3, 0.2]

Devices send updates:
  Device 1: weights_1 = [0.45, 0.35, 0.25], n_samples_1 = 500
  Device 2: weights_2 = [0.55, 0.25, 0.20], n_samples_2 = 400
  Device 3: weights_3 = [0.48, 0.32, 0.23], n_samples_3 = 300

Calculate weighted average:
  weights_global = (weights_1 × n_1 + weights_2 × n_2 + weights_3 × n_3)
                   / (n_1 + n_2 + n_3)
                 = ([0.45, 0.35, 0.25] × 500 + [0.55, 0.25, 0.20] × 400 + [0.48, 0.32, 0.23] × 300)
                   / 1200
                 = [0.494, 0.323, 0.223]

Global Model v2 → weights_global = [0.494, 0.323, 0.223]
```

**Why weighting by samples:**

-   Devices with more data should influence more
-   Device 1 (500 samples) has 2.5x influence of Device 3 (200 samples)
-   Fair representation across different-sized datasets

---

### 4. Model Evaluation (Spark)

**What it does:** Tests how accurate our model really is on new data

**Metrics:**

```
For 1000 test samples:

True Positives (TP): 600 (predicted anomaly, was anomaly) ✓
False Positives (FP): 50 (predicted anomaly, was normal) ✗
True Negatives (TN): 320 (predicted normal, was normal) ✓
False Negatives (FN): 30 (predicted normal, was anomaly) ✗

Accuracy = (TP + TN) / All = (600 + 320) / 1000 = 92.0%
Precision = TP / (TP + FP) = 600 / 650 = 92.3% (when we say anomaly, how often right?)
Recall = TP / (TP + FN) = 600 / 630 = 95.2% (of real anomalies, how many did we catch?)
F1 = 2×(Precision×Recall)/(Precision+Recall) = 93.7% (balanced metric)
```

**Why multiple metrics:**

-   **Accuracy:** Overall correctness (can be misleading if imbalanced)
-   **Precision:** Avoid false alarms (don't bother users with false alerts)
-   **Recall:** Catch real problems (don't miss actual anomalies)
-   **F1:** Balance both (harmonic mean)

---

## PERFORMANCE METRICS

### System Throughput

| Metric                 | Value  | Unit        |
| ---------------------- | ------ | ----------- |
| Message Ingestion Rate | 31,482 | msgs/minute |
| Local Models Created   | 11,247 | total       |
| Global Models Created  | 70     | total       |
| Average Training Time  | 2-5    | seconds     |
| Model Inference Time   | <1     | millisecond |



Observations:

-   v1-v10: Rapid improvement (devices learning)
-   v10-v40: Steady improvement (global model refining)
-   v40-v70: Stabilizing around 72-73% (convergence)
-   Some oscillations: Normal (different device batches contribute)

### Storage Efficiency

| Table             | Records | Size   | Growth Rate |
| ----------------- | ------- | ------ | ----------- |
| local_models      | 11,247  | ~5 MB  | ~50/min     |
| federated_models  | 70      | ~50 KB | ~0.5/hour   |
| model_evaluations | ~70,000 | ~30 MB | ~1000/hour  |
| anomalies         | ~30,000 | ~15 MB | ~500/min    |

**Compression by TimescaleDB:** ~90% reduction through hypertables

### Network Bandwidth Saved

**Traditional Approach:** Send all data to server

```
2,407 devices × 30 samples/minute × 100 bytes/sample
= 7.2 MB/minute upstream
```

**Federated Learning Approach:** Send only model updates

```
2,407 devices × 1 model update/minute × 200 bytes/update
= 0.48 MB/minute upstream
= 93% bandwidth reduction!
```

### Cost Savings (Estimated)

| Component   | Bandwidth | Compute | Storage   |
| ----------- | --------- | ------- | --------- |
| Traditional | $7.2/min  | High    | Very High |
| FLEAD       | $0.48/min | Low     | Medium    |
| **Savings** | **93%**   | **80%** | **50%**   |

---

## SUMMARY: THE COMPLETE PICTURE

**FLEAD brings together 6 technologies in perfect harmony:**

1. **Kafka** = Nervous system (routes messages)
2. **Flink** = Local brains (2,407 independent learners)
3. **FedAvg** = Collective intelligence (combines local knowledge)
4. **Spark** = Auditor (validates results)
5. **TimescaleDB** = Memory (stores everything)
6. **Grafana** = Eyes (visualizes progress)

**Data Journey:**

```
IoT Device
    ↓ (raw data)
Kafka
    ↓ (distributes)
Flink (trains locally)
    ↓ (model updates)
Federated Aggregation (FedAvg)
    ↓ (new global version)
TimescaleDB (stores)
    ↓ (queries)
Spark (evaluates)
    ↓ (writes results)
TimescaleDB (stores again)
    ↓ (queries)
Grafana (visualizes)
    ↓
**YOU SEE: Beautiful dashboard with 72.7% accuracy**
```

**Why this architecture?**

-   **Privacy:** Raw data never leaves devices
-   **Efficiency:** 93% less bandwidth
-   **Scalability:** Adds device = adds computation (not bottleneck)
-   **Resilience:** Device failure doesn't crash system
-   **Flexibility:** Easy to add new components
-   **Real-time:** Streaming + batch analysis
-   **Interpretability:** Can track every model version

**Key Achievement:**

> Successfully deployed federated learning on 2,407 IoT devices, achieving 72.7% anomaly detection accuracy while maintaining privacy and reducing bandwidth by 93%.

---

## QUICK REFERENCE TABLE

| Component       | Technology        | Role              | Input          | Output          |
| --------------- | ----------------- | ----------------- | -------------- | --------------- |
| **Source**      | IoT Sensors       | Data generation   | -              | Raw values      |
| **Kafka**       | Message Broker    | Data distribution | Raw data       | Topics          |
| **Flink**       | Stream Processing | Local training    | Topic msgs     | Model updates   |
| **FedAvg**      | Aggregation       | Global model      | Local updates  | Global model    |
| **Spark**       | Analytics         | Model evaluation  | Global models  | Accuracy scores |
| **TimescaleDB** | Time-series DB    | Data storage      | All components | SQL queries     |
| **Grafana**     | Visualization     | Dashboard         | SQL queries    | Live dashboard  |
