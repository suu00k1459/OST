services:
    # -------------------------------------------
    # Kafka - Message Broker (KRaft mode, no Zookeeper)
    # -------------------------------------------
    kafka:
        image: confluentinc/cp-kafka:7.6.1
        container_name: kafka
        ports:
            - "9092:9092"
            - "29092:29092"
        environment:
            # CLUSTER_ID is required - Kafka KRaft mode needs a cluster ID environment variable to bootstrap itself
            # No security protocol defined for listener CONTROLLER - Kafka was trying to use a CONTROLLER listener,
            # but the security protocol map didn't tell it what protocol to use.
            # It's like saying "listen on port 29093" but not saying "use PLAINTEXT" or "use SSL"
            CLUSTER_ID: 4L6g3nQlTjGEKK1RVAx_vQ
            KAFKA_BROKER_ID: 1
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
            KAFKA_PROCESS_ROLES: broker,controller
            KAFKA_NODE_ID: 1
            KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
            KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092
            KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
            KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
        volumes:
            - kafka_data:/var/lib/kafka/data
        networks:
            - flead_network
        healthcheck:
            test: ["CMD", "sh", "-c", "echo 'ok'"]
            interval: 5s
            timeout: 3s
            retries: 3
            start_period: 30s

    # -------------------------------------------
    # Kafka UI - Kafka Management Interface
    # -------------------------------------------
    kafka-ui:
        image: provectuslabs/kafka-ui:latest
        container_name: kafka-ui
        depends_on:
            kafka:
                condition: service_healthy
        ports:
            - "8081:8080"
        environment:
            KAFKA_CLUSTERS_0_NAME: flead-cluster
            KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
        networks:
            - flead_network

    # -------------------------------------------
    # TimescaleDB - Time-Series Database
    # -------------------------------------------
    timescaledb:
        image: timescale/timescaledb-ha:pg16
        container_name: timescaledb
        ports:
            - "5432:5432"
        environment:
            POSTGRES_PASSWORD: password
            POSTGRES_DB: flead
            POSTGRES_USER: flead
        volumes:
            - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
            - timescaledb_data:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U flead -d flead -h localhost"]
            interval: 5s
            timeout: 10s
            retries: 20
            start_period: 30s
        networks:
            - flead_network

    # -------------------------------------------
    # Grafana - Monitoring & Visualization
    # -------------------------------------------
    grafana:
        image: grafana/grafana:11.0.0
        container_name: grafana
        ports:
            - "3001:3000"
        environment:
            GF_SECURITY_ADMIN_USER: admin
            GF_SECURITY_ADMIN_PASSWORD: admin
        volumes:
            - ./grafana/provisioning:/etc/grafana/provisioning
            - ./grafana/dashboards:/var/lib/grafana/dashboards
        depends_on:
            timescaledb:
                condition: service_healthy
        networks:
            - flead_network
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
            interval: 10s
            timeout: 5s
            retries: 5

    # -------------------------------------------
    # Apache Flink - Stream Processing (Local Models)
    # NOTE: Custom build with Python and PyFlink installed
    # -------------------------------------------
    flink-jobmanager:
        build:
            context: .
            dockerfile: Dockerfile.flink
        container_name: flink-jobmanager
        command: jobmanager
        ports:
            - "6123:6123"
            - "8161:8081"
        environment:
            - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
            - JOB_MANAGER_RPC_PORT=6123
            - TASK_MANAGER_RPC_PORT=6122
            - TASK_MANAGER_MEMORY_PROCESS_SIZE=1024m
            - JOB_MANAGER_MEMORY_PROCESS_SIZE=1024m
        volumes:
            - flink_jobmanager_data:/flink/data
            - ./scripts:/opt/flink/scripts
            - ./data:/opt/flink/data
        networks:
            - flead_network
        depends_on:
            kafka:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8081/"]
            interval: 10s
            timeout: 5s
            retries: 5
            start_period: 60s

    flink-taskmanager:
        build:
            context: .
            dockerfile: Dockerfile.flink
        container_name: flink-taskmanager
        command: taskmanager
        depends_on:
            flink-jobmanager:
                condition: service_healthy
            kafka:
                condition: service_healthy
        environment:
            - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
            - JOB_MANAGER_RPC_PORT=6123
            - TASK_MANAGER_RPC_PORT=6122
            - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=4
            - TASK_MANAGER_MEMORY_PROCESS_SIZE=2048m
            - TASK_MANAGER_MEMORY_FRAMEWORK_HEAP_SIZE=512m
            - TASK_MANAGER_MEMORY_TASK_HEAP_SIZE=1024m
        volumes:
            - flink_taskmanager_data:/flink/data
            - ./scripts:/opt/flink/scripts
            - ./data:/opt/flink/data
        networks:
            - flead_network

    # -------------------------------------------
    # Apache Spark - Batch Analytics (Custom build with packages installed)
    # -------------------------------------------
    spark-master:
        build:
            context: .
            dockerfile: Dockerfile.spark
        container_name: spark-master
        command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
        environment:
            - SPARK_NO_DAEMONIZE=1
            - SPARK_MASTER_HOST=spark-master
            - SPARK_MASTER_PORT=7077
            - SPARK_MASTER_WEBUI_PORT=8080
        ports:
            - "8086:8080"
            - "7077:7077"
        volumes:
            - spark_master_data:/opt/spark/work
            - ./scripts:/opt/spark/scripts
            - ./data:/opt/spark/data
        networks:
            - flead_network
        depends_on:
            kafka:
                condition: service_healthy
            timescaledb:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/"]
            interval: 10s
            timeout: 5s
            retries: 5
            start_period: 60s
    spark-job:
        build:
            context: .
            dockerfile: Dockerfile.spark
        container_name: spark-job
        depends_on:
            spark-master:
                condition: service_healthy
        command: bash -c "sleep 60 && /opt/spark/bin/spark-submit --master spark://spark-master:7077 /opt/spark/scripts/spark_analytics.py"
        volumes:
            - ./scripts:/opt/spark/scripts
            - ./data:/opt/spark/data
        networks:
            - flead_network


    spark-worker-1:
        build:
            context: .
            dockerfile: Dockerfile.spark
        container_name: spark-worker-1
        command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
        environment:
            - SPARK_NO_DAEMONIZE=1
            - SPARK_WORKER_CORES=2
            - SPARK_WORKER_MEMORY=1
            - SPARK_MASTER=spark://spark-master:7077
        ports:
            - "8087:8081"
        volumes:
            - spark_worker_data:/opt/spark/work
            - ./scripts:/opt/spark/scripts
            - ./data:/opt/spark/data
        networks:
            - flead_network
        depends_on:
            spark-master:
                condition: service_healthy

volumes:
    kafka_data:
    timescaledb_data:
    flink_jobmanager_data:
    flink_taskmanager_data:
    spark_master_data:
    spark_worker_data:

networks:
    flead_network:
        driver: bridge
