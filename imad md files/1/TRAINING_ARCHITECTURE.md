# FLEAD Training Architecture & Improvement Strategy

## ğŸ¯ Current System Flow

### **Stage 1: Local Model Training (Flink)**

**File**: `scripts/03_flink_local_training.py`

```
IoT Device Data (Kafka: edge-iiot-stream)
    â†“
[Per-Device Window: 30-50 rows or 60 seconds]
    â†“
Local Model Training:
  â€¢ Z-score anomaly detection (on streaming data)
  â€¢ Rolling statistics (mean, std, 100-value window)
  â€¢ Model version incremented every 50 rows OR 60 seconds
    â†“
Output: local-model-updates topic (Kafka)
  - device_id
  - model_version
  - accuracy (CURRENTLY FIXED: 0.7 â†’ 0.95 via formula)
  - samples_processed
  - timestamp
```

**Current Accuracy Calculation:**

```python
accuracy = min(0.95, 0.7 + (model['version'] * 0.02))
# Version 1: 72%, Version 2: 74%, Version 3: 76%, ... Version 13: 94%, Version 14+: 95%
```

âŒ **Problem**: Accuracy is hardcoded, not trained from real data!

---

### **Stage 2: Federated Aggregation (Host Service)**

**File**: `scripts/04_federated_aggregation.py`

```
Local Model Updates (20 models collected)
    â†“
Federated Averaging (FedAvg):
  â€¢ Weighted average of local accuracies
  â€¢ Weight = samples_processed per device
  â€¢ Global Accuracy = Î£(local_accuracy Ã— samples) / Î£(samples)
    â†“
Global Model v1 â†’ v2 â†’ v3 ...
  - Stored in: GLOBAL_MODELS_DIR (pickle files)
  - Logged in: federated_models table (TimescaleDB)
    â†“
Output: global-model-updates topic (Kafka)
  - version
  - aggregation_round
  - global_accuracy
  - num_devices
  - timestamp
```

**Aggregation Trigger**: Every 20 local model updates

---

### **Stage 3: Spark Analytics (Docker)**

**File**: `scripts/05_spark_analytics.py`

```
Global Model Updates + Streaming Data
    â†“
Two Analysis Paths:

A) BATCH ANALYSIS (Daily, on historical data):
   â€¢ Compute daily stats per device
   â€¢ Calculate anomalies vs global model
   â€¢ Store in: batch_analysis_results table

B) STREAM ANALYSIS (Real-time, 30-5min windows):
   â€¢ Real-time anomaly detection
   â€¢ Compare to global model predictions
   â€¢ Store in: stream_analysis_results table
    â†“
Model Evaluation:
   â€¢ Load latest global model
   â€¢ Test on stream data
   â€¢ Record prediction results
   â€¢ Store in: model_evaluations table
    â†“
Dashboard Metrics:
   â€¢ Update Grafana with results
   â€¢ Show model performance over time
```

---

## ğŸ“Š Current Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IoT DEVICES (2,407)                          â”‚
â”‚              Streaming sensor data continuously                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  KAFKA TOPICS  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ edge-iiot-   â”‚
    â”‚   stream       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€ (12,000+ messages)
    â”‚ â€¢ local-model- â”‚
    â”‚   updates      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€(device models)
    â”‚ â€¢ global-model-â”‚
    â”‚   updates      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€(global model)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
      â–¼      â–¼      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      FLINK (Real-time, per-device)    â”‚
    â”‚  â€¢ Anomaly detection (Z-score)        â”‚
    â”‚  â€¢ Local model training (every 50 rows)
    â”‚  â€¢ Accuracy: 0.7-0.95 (FIXED FORMULA) â”‚ âŒ NOT TRAINED
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  FEDERATED AGGREGATION (Host)      â”‚
    â”‚  â€¢ Collect 20 local models         â”‚
    â”‚  â€¢ Weighted average accuracy       â”‚
    â”‚  â€¢ Create global model v1 â†’ v2 ... â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SPARK ANALYTICS (Docker)            â”‚
    â”‚  â€¢ Batch: Daily analysis               â”‚
    â”‚  â€¢ Stream: Real-time eval              â”‚
    â”‚  â€¢ Evaluate global model accuracy      â”‚ â† POINT TO ADD FEEDBACK
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TIMESCALEDB (Storage)            â”‚
    â”‚  Tables:                            â”‚
    â”‚  â€¢ local_models                    â”‚
    â”‚  â€¢ federated_models                â”‚
    â”‚  â€¢ model_evaluations               â”‚ â† Results here
    â”‚  â€¢ batch_analysis_results          â”‚
    â”‚  â€¢ stream_analysis_results         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   GRAFANA (Visualization)          â”‚
    â”‚  â€¢ Show model accuracy trends      â”‚
    â”‚  â€¢ Visualize anomalies             â”‚
    â”‚  â€¢ Track device performance        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”´ WHY ACCURACY IS STUCK AT 74%

Looking at `03_flink_local_training.py` line 147:

```python
accuracy = min(0.95, 0.7 + (model['version'] * 0.02))
```

**This is NOT real training!**

-   Version 1: 0.7 + 0.02 = **72%**
-   Version 2: 0.7 + 0.04 = **74%** â† You're seeing this
-   Version 3: 0.7 + 0.06 = **76%**
-   ...
-   Version 13: 0.7 + 0.26 = **96%**
-   Version 14+: **95%** (capped)

**The accuracy is just increasing by 2% per version!** Real training would:

1. Calculate actual loss/error from predictions
2. Use SGD/Adam to update weights
3. Track validation accuracy
4. Stop improving if accuracy plateaus (overfitting)

---

## âœ… WHAT'S WORKING WELL

1. **Data Pipeline**: âœ… Streaming data flowing smoothly
2. **Federated Aggregation**: âœ… Proper FedAvg implementation
3. **Storage**: âœ… All data in TimescaleDB
4. **Spark Analysis**: âœ… Batch + Stream processing works
5. **Database**: âœ… model_evaluations table has real evaluation results

---

## ğŸš€ HOW TO ADD IMPROVEMENT LOOPS (Backpropagation)

### **Option 1: Simple Feedback Loop (Easy, Fast)**

Add a feedback mechanism after Spark evaluation:

```
Spark Results â†’ Evaluate Prediction Accuracy
    â†“
Calculate Error: actual_value - predicted_value
    â†“
Store in: model_feedback table
    â†“
Flink reads feedback â†’ Adjusts next local model
```

**Implementation**:

1. Create `model_feedback` table in TimescaleDB
2. Spark writes evaluation results there
3. Flink reads feedback to improve training

---

### **Option 2: Real Gradient-Based Training (Medium, Realistic)**

Replace the hardcoded accuracy formula with actual SGD:

```python
# CURRENT (fake):
accuracy = min(0.95, 0.7 + (model['version'] * 0.02))

# IMPROVED (real training):
# Use scikit-learn or PyTorch for actual model training
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier

def train_local_model(training_data, global_model_weights):
    """
    Real local model training with global model initialization
    """
    X, y = training_data

    # Initialize with global model weights (if available)
    model = SGDClassifier(loss='log')
    if global_model_weights is not None:
        model.coef_ = global_model_weights

    # Train on local data (SGD = stochastic gradient descent)
    model.partial_fit(X, y, classes=[0, 1])

    # Calculate real accuracy
    accuracy = model.score(X, y)  # Real!

    return model, accuracy
```

---

### **Option 3: Closed-Loop Improvement (Advanced, Full System)**

**CREATE A FEEDBACK CYCLE:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CURRENT STATE (Open Loop):                          â”‚
â”‚ Flink â†’ Federated â†’ Spark â†’ Database â†’ Grafana     â”‚
â”‚ (No feedback, models don't improve from results)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         â”‚
                         â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IMPROVED STATE (Closed Loop):                       â”‚
â”‚                                                     â”‚
â”‚ 1. Flink trains local model                        â”‚
â”‚    â†“                                                â”‚
â”‚ 2. Federated aggregates â†’ global model             â”‚
â”‚    â†“                                                â”‚
â”‚ 3. Spark evaluates on new data                     â”‚
â”‚    â†“                                                â”‚
â”‚ 4. Calculate error & loss metrics                  â”‚
â”‚    â†“                                                â”‚
â”‚ 5. [NEW] Write feedback to Kafka topic             â”‚
â”‚    â†“                                                â”‚
â”‚ 6. [NEW] Flink reads feedback                      â”‚
â”‚    â†“                                                â”‚
â”‚ 7. [NEW] Adjust next local model training          â”‚
â”‚    â†“                                                â”‚
â”‚ 8. Loop back to step 1 (improved!)                â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Implementation Roadmap (Easiest to Hardest)

### **Phase 1: Add Evaluation Feedback (1 hour)**

âœ… **What**: Spark evaluations â†’ feedback table â†’ logs

**Steps**:

1. Create `model_feedback` table:

```sql
CREATE TABLE model_feedback (
    id BIGSERIAL PRIMARY KEY,
    model_version INT,
    device_id TEXT,
    evaluation_timestamp TIMESTAMPTZ,
    prediction FLOAT,
    actual FLOAT,
    error FLOAT,
    confidence FLOAT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

2. Spark writes feedback:

```python
# In 05_spark_analytics.py
def save_feedback(evaluations):
    for eval in evaluations:
        error = abs(eval['actual'] - eval['prediction'])
        # Store in model_feedback table
```

3. Monitor improvements:

```sql
SELECT model_version, AVG(ABS(error)) as avg_error
FROM model_feedback
GROUP BY model_version
ORDER BY model_version;
```

**Result**: See which global model versions perform best!

---

### **Phase 2: Adaptive Learning Rate (2 hours)**

**What**: Adjust training based on feedback

```python
# In 03_flink_local_training.py
def adaptive_accuracy_calculation(model_version, feedback_data):
    """
    Calculate accuracy based on actual feedback, not hardcoded formula
    """
    if feedback_data:
        # Real accuracy from previous evaluations
        avg_error = np.mean([f['error'] for f in feedback_data])
        actual_accuracy = 1.0 - avg_error  # Better metric!
    else:
        # Fallback during startup
        actual_accuracy = min(0.95, 0.7 + (model_version * 0.01))

    return actual_accuracy
```

---

### **Phase 3: True SGD Training (4 hours)**

**What**: Replace formula with real machine learning

```python
# Replace hardcoded formula with:
from sklearn.linear_model import SGDClassifier
import numpy as np

class RealLocalModel:
    def __init__(self, global_weights=None):
        self.model = SGDClassifier(loss='log', learning_rate='optimal')
        if global_weights is not None:
            self.model.coef_ = global_weights

    def train(self, X, y):
        """Incremental learning (perfect for streaming)"""
        self.model.partial_fit(X, y, classes=[0, 1])
        accuracy = self.model.score(X, y)
        return accuracy, self.model.coef_

    def predict(self, X):
        return self.model.predict(X)
```

---

### **Phase 4: Full Closed-Loop (6 hours)**

**What**: Complete feedback cycle

```
Kafka topic: "model-feedback"
    â†“
Flink reads feedback every 10 models
    â†“
If accuracy improving â†’ keep current approach
If accuracy stuck â†’ try different hyperparameters
    â†“
Retry with feedback â†’ send new model
```

---

## ğŸ¯ QUICK WIN: Add Real Accuracy Tracking

**Current Problem**:

```python
accuracy = min(0.95, 0.7 + (model['version'] * 0.02))  # FAKE
```

**Quick Fix** (replace with real evaluation):

```python
# In 03_flink_local_training.py

def calculate_real_accuracy(device_id, current_stats):
    """
    Instead of hardcoded formula, use actual model performance
    """
    # Get recent predictions from this device
    recent_predictions = get_device_predictions(device_id, limit=100)

    if recent_predictions:
        # Calculate real accuracy from actual vs predicted
        correct = sum(1 for p in recent_predictions if p['is_correct'])
        accuracy = correct / len(recent_predictions)
    else:
        # First model, estimate based on training data fit
        mean_error = abs(current_stats['mean'] - current_stats['rolling_mean'])
        accuracy = 1.0 - min(mean_error / current_stats['std'], 1.0)

    return min(0.95, max(0.5, accuracy))  # Cap between 50-95%
```

---

## ğŸ“Š Monitoring Improvement

**Dashboard Queries** (add to Grafana):

1. **Model Accuracy Over Time**:

```sql
SELECT model_version, avg(accuracy)
FROM federated_models
GROUP BY model_version
ORDER BY created_at;
```

2. **Feedback Quality**:

```sql
SELECT model_version,
       AVG(ABS(error)) as avg_error,
       COUNT(*) as evaluations
FROM model_feedback
GROUP BY model_version
ORDER BY model_version DESC;
```

3. **Improvement Rate**:

```sql
WITH ranked AS (
    SELECT model_version, accuracy,
           LAG(accuracy) OVER (ORDER BY created_at) as prev_accuracy
    FROM federated_models
)
SELECT model_version,
       (accuracy - prev_accuracy) as improvement
FROM ranked
WHERE prev_accuracy IS NOT NULL
ORDER BY model_version DESC;
```

---

## ğŸš¦ Recommended Implementation Order

1. **Start**: Add Phase 1 (Feedback table) - Today!
2. **Next**: Phase 2 (Adaptive learning) - Tomorrow
3. **Later**: Phase 3 (Real SGD) - Next week
4. **Final**: Phase 4 (Full loop) - When needed

---

## ğŸ“Œ Summary

| Component      | Current           | Issue         | Solution                   |
| -------------- | ----------------- | ------------- | -------------------------- |
| Flink Training | Hardcoded formula | Fake accuracy | Real loss calculation      |
| Accuracy       | Stuck at 74%      | Not learning  | Base on actual predictions |
| Feedback       | None              | No loop       | Create feedback table      |
| Improvement    | Manual            | Static        | Automated adjustment       |
| Global Model   | WeightedAvg       | Simple        | Include quality signals    |

**Next Step**: Would you like me to implement Phase 1 (Feedback Loop) right now? Takes 30 minutes!

---

_Last Updated: November 7, 2025_
_Ready to add real training loops!_
