{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a313a470",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing - Kaggle Environment\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Loading the Edge-IIoT dataset (already available in Kaggle environment)\n",
    "2. Loading and analyzing the datasets\n",
    "3. Data cleaning and validation\n",
    "4. Merging datasets and organizing by device\n",
    "5. Exporting processed data for streaming\n",
    "\n",
    "Dataset: [Edge-IIoT Set Dataset](https://www.kaggle.com/datasets/sibasispradhan/edge-iiotset-dataset)\n",
    "\n",
    "The Edge-IIoT dataset contains sensor data from IoT edge devices for anomaly detection research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0983b",
   "metadata": {},
   "source": [
    "## 1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69628cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pandas numpy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fab486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f5ca9",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from Kaggle Input\n",
    "\n",
    "In Kaggle, the dataset is automatically available in `/kaggle/input/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1da1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset path - automatically available in the environment\n",
    "data_dir = Path('/kaggle/input/edge-iiotset-dataset')\n",
    "\n",
    "# Verify the directory exists\n",
    "if data_dir.exists():\n",
    "    print(f\"Dataset directory found: {data_dir}\")\n",
    "    print(f\"\\nFiles in dataset:\")\n",
    "    for file in sorted(data_dir.glob('*.csv')):\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"ERROR: Dataset not found at {data_dir}\")\n",
    "    print(\"Make sure the Edge-IIoT dataset is added to your Kaggle notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2b7de",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Datasets\n",
    "\n",
    "Load all CSV files and analyze their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(data_dir.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "\n",
    "datasets = {}\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Loading {csv_file.name}...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    datasets[csv_file.stem] = df\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "print(f\"Loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nDimensions: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nMissing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad66e3",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "**Remove duplicates, handle missing values, and convert data types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64261571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, name):\n",
    "    \"\"\"Clean and validate dataset\"\"\"\n",
    "    print(f\"Cleaning {name}...\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df.drop_duplicates().reset_index(drop=True)\n",
    "    duplicates = initial_rows - len(df_clean)\n",
    "    if duplicates > 0:\n",
    "        print(f\"  Removed {duplicates} duplicate rows\")\n",
    "    \n",
    "    # Drop rows with any missing values\n",
    "    rows_before = len(df_clean)\n",
    "    df_clean = df_clean.dropna()\n",
    "    missing_removed = rows_before - len(df_clean)\n",
    "    if missing_removed > 0:\n",
    "        print(f\"  Removed {missing_removed} rows with missing values\")\n",
    "    \n",
    "    # Convert object columns to numeric where possible\n",
    "    converted_success = 0\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            try:\n",
    "                converted = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                if converted.notna().sum() > 0:\n",
    "                    df_clean[col] = converted\n",
    "                    converted_success += 1\n",
    "            except:\n",
    "                pass\n",
    "    if converted_success > 0:\n",
    "        print(f\"  Converted {converted_success} object columns to numeric\")\n",
    "    \n",
    "    # Add source dataset indicator\n",
    "    df_clean['dataset_source'] = name\n",
    "    \n",
    "    print(f\"  Final: {len(df_clean):,} rows ({100*len(df_clean)/initial_rows:.1f}% retained)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "cleaned_datasets = {}\n",
    "for name, df in datasets.items():\n",
    "    cleaned_datasets[name] = clean_dataset(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2ee55",
   "metadata": {},
   "source": [
    "## 5. Merge Datasets\n",
    "\n",
    "Combine all cleaned datasets into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee50952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging datasets...\")\n",
    "\n",
    "# Analyze columns across datasets\n",
    "all_columns = set()\n",
    "for df in cleaned_datasets.values():\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "print(f\"Total unique columns across all datasets: {len(all_columns)}\")\n",
    "\n",
    "# Find common columns\n",
    "common_cols = set(cleaned_datasets[list(cleaned_datasets.keys())[0]].columns)\n",
    "for df in list(cleaned_datasets.values())[1:]:\n",
    "    common_cols &= set(df.columns)\n",
    "\n",
    "print(f\"Columns in all datasets: {len(common_cols)}\")\n",
    "print(f\"Common columns: {sorted(common_cols)}\\n\")\n",
    "\n",
    "# Merge all datasets\n",
    "df_merged = pd.concat(cleaned_datasets.values(), ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"Merged dataset shape: {df_merged.shape}\")\n",
    "print(f\"Total rows: {len(df_merged):,}\")\n",
    "print(f\"Total columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(f\"\\nDataset sources distribution:\")\n",
    "print(df_merged['dataset_source'].value_counts())\n",
    "\n",
    "# Calculate data sparsity\n",
    "total_cells = df_merged.shape[0] * df_merged.shape[1]\n",
    "non_null = df_merged.notna().sum().sum()\n",
    "sparsity = (1 - non_null / total_cells) * 100\n",
    "\n",
    "print(f\"\\nData quality metrics:\")\n",
    "print(f\"  Total cells: {total_cells:,}\")\n",
    "print(f\"  Non-null cells: {non_null:,}\")\n",
    "print(f\"  Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "print(f\"\\nMissing values per column (top 10):\")\n",
    "missing_per_col = df_merged.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_per_col.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9509058",
   "metadata": {},
   "source": [
    "## 6. Device Identification and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a39a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing device/identifier columns\n",
    "print(\"Looking for device identifier columns...\")\n",
    "device_col_candidates = ['device_id', 'Device_ID', 'DeviceID', 'device', 'Device', 'id', 'ID', 'src_ip', 'dst_ip', 'ip.src', 'ip.dst']\n",
    "\n",
    "device_col = None\n",
    "for col in device_col_candidates:\n",
    "    if col in df_merged.columns:\n",
    "        # Check if column has meaningful values (not all NaN)\n",
    "        if df_merged[col].notna().sum() > 0:\n",
    "            device_col = col\n",
    "            print(f\"Found column: {col}\")\n",
    "            break\n",
    "\n",
    "if device_col is None:\n",
    "    print(\"No device identifier column found\")\n",
    "    print(f\"Using dataset source + random grouping instead\\n\")\n",
    "    \n",
    "    # Group by dataset source and create device IDs within each\n",
    "    num_devices = max(5, len(df_merged) // 1000)\n",
    "    df_merged['device_id'] = df_merged.groupby('dataset_source').cumcount() % num_devices\n",
    "    print(f\"Created {num_devices} synthetic device IDs per dataset\")\n",
    "else:\n",
    "    print(f\"Using existing device column: {device_col}\")\n",
    "    df_merged.rename(columns={device_col: 'device_id'}, inplace=True)\n",
    "\n",
    "print(f\"\\nDevices distribution:\")\n",
    "print(df_merged['device_id'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDevice counts by dataset:\")\n",
    "print(df_merged.groupby(['dataset_source', 'device_id']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grouping data by device...\")\n",
    "\n",
    "device_groups = {}\n",
    "for device_id, group in df_merged.groupby('device_id', sort=False):\n",
    "    device_groups[str(device_id)] = group.copy(deep=False)\n",
    "\n",
    "print(f\"Created {len(device_groups)} device groups\\n\")\n",
    "\n",
    "print(\"Device group statistics:\")\n",
    "print(\"-\" * 60)\n",
    "for device_id, group in sorted(device_groups.items()):\n",
    "    print(f\"Device {device_id}: {len(group)} rows\")\n",
    "\n",
    "print(f\"\\nData grouped by device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74daf2",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save preprocessed data and statistics to Kaggle output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Kaggle, save outputs to /kaggle/working/\n",
    "output_dir = Path('/kaggle/working/edge_iiot_processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Exporting processed data to {output_dir}\\n\")\n",
    "\n",
    "# Export merged dataset\n",
    "merged_file = output_dir / 'merged_data.csv'\n",
    "df_merged.to_csv(merged_file, index=False)\n",
    "print(f\"Merged data: {merged_file}\")\n",
    "print(f\"  Size: {merged_file.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# Export device-specific files\n",
    "print(f\"\\nDevice files:\")\n",
    "for device_id, df_device in device_groups.items():\n",
    "    device_file = output_dir / f'device_{device_id}.csv'\n",
    "    df_device.to_csv(device_file, index=False)\n",
    "    print(f\"  device_{device_id}.csv ({len(df_device)} rows)\")\n",
    "\n",
    "# Export summary\n",
    "summary = {\n",
    "    'total_records': len(df_merged),\n",
    "    'total_devices': len(device_groups),\n",
    "    'total_columns': len(df_merged.columns),\n",
    "    'dataset_sources': df_merged['dataset_source'].value_counts().to_dict(),\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "summary_file = output_dir / 'processing_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nExport complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7dd951",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed the following steps:\n",
    "\n",
    "1. Loaded the Edge-IIoT dataset from Kaggle environment\n",
    "2. Loaded and analyzed three CSV files\n",
    "3. Cleaned data by removing duplicates and handling missing values\n",
    "4. Merged datasets into a consolidated dataframe\n",
    "5. Organized data by device identifier\n",
    "6. Exported processed data to multiple formats\n",
    "\n",
    "### Output files\n",
    "- `merged_data.csv` - Complete merged dataset\n",
    "- `device_*.csv` - Per-device data files  \n",
    "- `processing_summary.json` - Processing statistics\n",
    "\n",
    "All outputs are saved to `/kaggle/working/edge_iiot_processed/` and ready for download."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
