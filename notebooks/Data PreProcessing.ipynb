{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ef963d",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing \n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Downloading the Edge-IIoT dataset from Kaggle\n",
    "2. Loading and analyzing the datasets\n",
    "3. Data cleaning and validation\n",
    "4. Merging datasets and organizing by device\n",
    "5. Converting data to streaming format\n",
    "6. Exporting processed data for streaming\n",
    "\n",
    "Dataset: [Edge-IIoT Set Dataset](https://www.kaggle.com/datasets/sibasispradhan/edge-iiotset-dataset)\n",
    "\n",
    "The Edge-IIoT dataset contains sensor data from IoT edge devices for anomaly detection research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12bccb",
   "metadata": {},
   "source": [
    "**Preprocessing Kaggle Notebook Link:** [Preprocessing](https://www.kaggle.com/code/imedbenmadi/federated-learning-preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec804d7",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d6547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet kaggle pandas numpy kafka-python python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d102cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.path.expanduser('~/.kaggle')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b7e41",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e15b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\imadb\\OneDrive\\Bureau\\OST Project\\notebooks\\edge_iiot_data\n",
      "Kaggle API authenticated\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "data_dir = Path('./edge_iiot_data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir.absolute()}\")\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "print(\"Kaggle API authenticated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426bbefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files already exist locally\n",
      "\n",
      "Files in edge_iiot_data:\n",
      "  DNN-EdgeIIoT-dataset.csv (1161.02 MB)\n",
      "  live_data_training.csv (96.99 MB)\n",
      "  ML-EdgeIIoT-dataset.csv (78.38 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check if files already exist\n",
    "csv_files = list(data_dir.glob('*.csv'))\n",
    "if len(csv_files) >= 3:\n",
    "    print(\"Dataset files already exist locally\\n\")\n",
    "    print(f\"Files in {data_dir}:\")\n",
    "    for file in sorted(csv_files):\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"Downloading Edge-IIoT dataset from Kaggle...\")\n",
    "    print(\"This may take several minutes...\\n\")\n",
    "    \n",
    "    api.dataset_download_files(\n",
    "        'sibasispradhan/edge-iiotset-dataset',\n",
    "        path=data_dir,\n",
    "        unzip=True\n",
    "    )\n",
    "    \n",
    "    print(\"Dataset downloaded successfully\\n\")\n",
    "    print(f\"Files in {data_dir}:\")\n",
    "    for file in sorted(data_dir.glob('*.csv')):\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {file.name} ({size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b302c2",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Datasets\n",
    "\n",
    "Load all CSV files and analyze their structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc63d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV files\n",
      "\n",
      "Loading DNN-EdgeIIoT-dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imadb\\AppData\\Local\\Temp\\ipykernel_19872\\1569496421.py:7: DtypeWarning: Columns (2,3,6,11,13,14,15,16,17,31,32,34,39,45,51,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (2219201, 63)\n",
      "  Columns: 63\n",
      "  Memory: 3330.01 MB\n",
      "\n",
      "Loading live_data_training.csv...\n",
      "  Memory: 3330.01 MB\n",
      "\n",
      "Loading live_data_training.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imadb\\AppData\\Local\\Temp\\ipykernel_19872\\1569496421.py:7: DtypeWarning: Columns (11,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (32481, 63)\n",
      "  Columns: 63\n",
      "  Memory: 120.53 MB\n",
      "\n",
      "Loading ML-EdgeIIoT-dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imadb\\AppData\\Local\\Temp\\ipykernel_19872\\1569496421.py:7: DtypeWarning: Columns (3,6,11,13,14,15,16,17,31,32,34,39,45,51,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (157800, 63)\n",
      "  Columns: 63\n",
      "  Memory: 235.52 MB\n",
      "\n",
      "Loaded 3 datasets\n",
      "  Memory: 235.52 MB\n",
      "\n",
      "Loaded 3 datasets\n"
     ]
    }
   ],
   "source": [
    "csv_files = sorted(data_dir.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "\n",
    "datasets = {}\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Loading {csv_file.name}...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    datasets[csv_file.stem] = df\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "print(f\"Loaded {len(datasets)} datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70da5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset: DNN-EdgeIIoT-dataset\n",
      "============================================================\n",
      "\n",
      "Dimensions: 2219201 rows x 63 columns\n",
      "\n",
      "Data types:\n",
      "frame.time             object\n",
      "ip.src_host            object\n",
      "ip.dst_host            object\n",
      "arp.dst.proto_ipv4     object\n",
      "arp.opcode            float64\n",
      "                       ...   \n",
      "mbtcp.len             float64\n",
      "mbtcp.trans_id        float64\n",
      "mbtcp.unit_id         float64\n",
      "Attack_label            int64\n",
      "Attack_type            object\n",
      "Length: 63, dtype: object\n",
      "\n",
      "First few rows:\n",
      "                  frame.time    ip.src_host    ip.dst_host arp.dst.proto_ipv4  \\\n",
      "0   2021 11:44:10.081753000   192.168.0.128  192.168.0.101                  0   \n",
      "1   2021 11:44:10.162218000   192.168.0.101  192.168.0.128                  0   \n",
      "2   2021 11:44:10.162271000   192.168.0.128  192.168.0.101                  0   \n",
      "3   2021 11:44:10.162641000   192.168.0.128  192.168.0.101                  0   \n",
      "4   2021 11:44:10.166132000   192.168.0.101  192.168.0.128                  0   \n",
      "\n",
      "   arp.opcode  arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
      "0         0.0          0.0                  0            0.0          0.0   \n",
      "1         0.0          0.0                  0            0.0          0.0   \n",
      "2         0.0          0.0                  0            0.0          0.0   \n",
      "3         0.0          0.0                  0            0.0          0.0   \n",
      "4         0.0          0.0                  0            0.0          0.0   \n",
      "\n",
      "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  \\\n",
      "0                      0.0  ...             0.0              0   \n",
      "1                      0.0  ...             4.0           MQTT   \n",
      "2                      0.0  ...             0.0              0   \n",
      "3                      0.0  ...             0.0              0   \n",
      "4                      0.0  ...             0.0              0   \n",
      "\n",
      "                 mqtt.topic mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id  \\\n",
      "0                         0            0.0      0.0       0.0            0.0   \n",
      "1                         0            0.0      4.0       0.0            0.0   \n",
      "2                         0            0.0      0.0       0.0            0.0   \n",
      "3                         0            0.0      0.0       0.0            0.0   \n",
      "4  Temperature_and_Humidity           24.0      0.0       0.0            0.0   \n",
      "\n",
      "  mbtcp.unit_id  Attack_label  Attack_type  \n",
      "0           0.0             0       Normal  \n",
      "1           0.0             0       Normal  \n",
      "2           0.0             0       Normal  \n",
      "3           0.0             0       Normal  \n",
      "4           0.0             0       Normal  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "\n",
      "Missing values:\n",
      "None\n",
      "\n",
      "============================================================\n",
      "Dataset: live_data_training\n",
      "============================================================\n",
      "\n",
      "Dimensions: 32481 rows x 63 columns\n",
      "\n",
      "Data types:\n",
      "frame.time            float64\n",
      "ip.src_host            object\n",
      "ip.dst_host            object\n",
      "arp.dst.proto_ipv4      int64\n",
      "arp.opcode              int64\n",
      "                       ...   \n",
      "mbtcp.len               int64\n",
      "mbtcp.trans_id          int64\n",
      "mbtcp.unit_id           int64\n",
      "Attack_label            int64\n",
      "Attack_type            object\n",
      "Length: 63, dtype: object\n",
      "\n",
      "First few rows:\n",
      "     frame.time ip.src_host ip.dst_host  arp.dst.proto_ipv4  arp.opcode  \\\n",
      "0  1.707159e+09           0           0                   0           0   \n",
      "1  1.707159e+09           0           0                   0           0   \n",
      "2  1.707159e+09           0           0                   0           0   \n",
      "3  1.707159e+09           0           0                   0           0   \n",
      "4  1.707159e+09           0           0                   0           0   \n",
      "\n",
      "   arp.hw.size  arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
      "0            0                   0              0            0   \n",
      "1            0                   0              0            0   \n",
      "2            0                   0              0            0   \n",
      "3            0                   0              0            0   \n",
      "4            0                   0              0            0   \n",
      "\n",
      "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
      "0                        0  ...               0              0           0   \n",
      "1                        0  ...               0              0           0   \n",
      "2                        0  ...               0              0           0   \n",
      "3                        0  ...               0              0           0   \n",
      "4                        0  ...               0              0           0   \n",
      "\n",
      "   mqtt.topic_len  mqtt.ver  mbtcp.len  mbtcp.trans_id  mbtcp.unit_id  \\\n",
      "0               0         0          0               0              0   \n",
      "1               0         0          0               0              0   \n",
      "2               0         0          0               0              0   \n",
      "3               0         0          0               0              0   \n",
      "4               0         0          0               0              0   \n",
      "\n",
      "  Attack_label    Attack_type  \n",
      "0            1      DDoS_ICMP  \n",
      "1            1       DDoS_UDP  \n",
      "2            1      DDoS_ICMP  \n",
      "3            1     Ransomware  \n",
      "4            1  Port_Scanning  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "\n",
      "Missing values:\n",
      "None\n",
      "\n",
      "============================================================\n",
      "Dataset: ML-EdgeIIoT-dataset\n",
      "============================================================\n",
      "\n",
      "Dimensions: 157800 rows x 63 columns\n",
      "\n",
      "Data types:\n",
      "frame.time             object\n",
      "ip.src_host            object\n",
      "ip.dst_host            object\n",
      "arp.dst.proto_ipv4     object\n",
      "arp.opcode            float64\n",
      "                       ...   \n",
      "mbtcp.len             float64\n",
      "mbtcp.trans_id        float64\n",
      "mbtcp.unit_id         float64\n",
      "Attack_label            int64\n",
      "Attack_type            object\n",
      "Length: 63, dtype: object\n",
      "\n",
      "First few rows:\n",
      "  frame.time    ip.src_host ip.dst_host arp.dst.proto_ipv4  arp.opcode  \\\n",
      "0        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "1        6.0  192.168.0.101         0.0                0.0         0.0   \n",
      "2        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "3        6.0  192.168.0.101         0.0                0.0         0.0   \n",
      "4        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "\n",
      "   arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
      "0          0.0                0.0            0.0          0.0   \n",
      "1          0.0                0.0            0.0          0.0   \n",
      "2          0.0                0.0            0.0          0.0   \n",
      "3          0.0                0.0            0.0          0.0   \n",
      "4          0.0                0.0            0.0          0.0   \n",
      "\n",
      "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
      "0                      0.0  ...             0.0            0.0         0.0   \n",
      "1                      0.0  ...             0.0            0.0         0.0   \n",
      "2                      0.0  ...             0.0            0.0         0.0   \n",
      "3                      0.0  ...             0.0            0.0         0.0   \n",
      "4                      0.0  ...             0.0            0.0         0.0   \n",
      "\n",
      "  mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id mbtcp.unit_id  \\\n",
      "0            0.0      0.0       0.0            0.0           0.0   \n",
      "1            0.0      0.0       0.0            0.0           0.0   \n",
      "2            0.0      0.0       0.0            0.0           0.0   \n",
      "3            0.0      0.0       0.0            0.0           0.0   \n",
      "4            0.0      0.0       0.0            0.0           0.0   \n",
      "\n",
      "   Attack_label  Attack_type  \n",
      "0             1         MITM  \n",
      "1             1         MITM  \n",
      "2             1         MITM  \n",
      "3             1         MITM  \n",
      "4             1         MITM  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "\n",
      "Missing values:\n",
      "None\n",
      "None\n",
      "\n",
      "============================================================\n",
      "Dataset: live_data_training\n",
      "============================================================\n",
      "\n",
      "Dimensions: 32481 rows x 63 columns\n",
      "\n",
      "Data types:\n",
      "frame.time            float64\n",
      "ip.src_host            object\n",
      "ip.dst_host            object\n",
      "arp.dst.proto_ipv4      int64\n",
      "arp.opcode              int64\n",
      "                       ...   \n",
      "mbtcp.len               int64\n",
      "mbtcp.trans_id          int64\n",
      "mbtcp.unit_id           int64\n",
      "Attack_label            int64\n",
      "Attack_type            object\n",
      "Length: 63, dtype: object\n",
      "\n",
      "First few rows:\n",
      "     frame.time ip.src_host ip.dst_host  arp.dst.proto_ipv4  arp.opcode  \\\n",
      "0  1.707159e+09           0           0                   0           0   \n",
      "1  1.707159e+09           0           0                   0           0   \n",
      "2  1.707159e+09           0           0                   0           0   \n",
      "3  1.707159e+09           0           0                   0           0   \n",
      "4  1.707159e+09           0           0                   0           0   \n",
      "\n",
      "   arp.hw.size  arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
      "0            0                   0              0            0   \n",
      "1            0                   0              0            0   \n",
      "2            0                   0              0            0   \n",
      "3            0                   0              0            0   \n",
      "4            0                   0              0            0   \n",
      "\n",
      "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
      "0                        0  ...               0              0           0   \n",
      "1                        0  ...               0              0           0   \n",
      "2                        0  ...               0              0           0   \n",
      "3                        0  ...               0              0           0   \n",
      "4                        0  ...               0              0           0   \n",
      "\n",
      "   mqtt.topic_len  mqtt.ver  mbtcp.len  mbtcp.trans_id  mbtcp.unit_id  \\\n",
      "0               0         0          0               0              0   \n",
      "1               0         0          0               0              0   \n",
      "2               0         0          0               0              0   \n",
      "3               0         0          0               0              0   \n",
      "4               0         0          0               0              0   \n",
      "\n",
      "  Attack_label    Attack_type  \n",
      "0            1      DDoS_ICMP  \n",
      "1            1       DDoS_UDP  \n",
      "2            1      DDoS_ICMP  \n",
      "3            1     Ransomware  \n",
      "4            1  Port_Scanning  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "\n",
      "Missing values:\n",
      "None\n",
      "\n",
      "============================================================\n",
      "Dataset: ML-EdgeIIoT-dataset\n",
      "============================================================\n",
      "\n",
      "Dimensions: 157800 rows x 63 columns\n",
      "\n",
      "Data types:\n",
      "frame.time             object\n",
      "ip.src_host            object\n",
      "ip.dst_host            object\n",
      "arp.dst.proto_ipv4     object\n",
      "arp.opcode            float64\n",
      "                       ...   \n",
      "mbtcp.len             float64\n",
      "mbtcp.trans_id        float64\n",
      "mbtcp.unit_id         float64\n",
      "Attack_label            int64\n",
      "Attack_type            object\n",
      "Length: 63, dtype: object\n",
      "\n",
      "First few rows:\n",
      "  frame.time    ip.src_host ip.dst_host arp.dst.proto_ipv4  arp.opcode  \\\n",
      "0        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "1        6.0  192.168.0.101         0.0                0.0         0.0   \n",
      "2        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "3        6.0  192.168.0.101         0.0                0.0         0.0   \n",
      "4        6.0  192.168.0.152         0.0                0.0         0.0   \n",
      "\n",
      "   arp.hw.size arp.src.proto_ipv4  icmp.checksum  icmp.seq_le  \\\n",
      "0          0.0                0.0            0.0          0.0   \n",
      "1          0.0                0.0            0.0          0.0   \n",
      "2          0.0                0.0            0.0          0.0   \n",
      "3          0.0                0.0            0.0          0.0   \n",
      "4          0.0                0.0            0.0          0.0   \n",
      "\n",
      "   icmp.transmit_timestamp  ...  mqtt.proto_len mqtt.protoname  mqtt.topic  \\\n",
      "0                      0.0  ...             0.0            0.0         0.0   \n",
      "1                      0.0  ...             0.0            0.0         0.0   \n",
      "2                      0.0  ...             0.0            0.0         0.0   \n",
      "3                      0.0  ...             0.0            0.0         0.0   \n",
      "4                      0.0  ...             0.0            0.0         0.0   \n",
      "\n",
      "  mqtt.topic_len mqtt.ver mbtcp.len mbtcp.trans_id mbtcp.unit_id  \\\n",
      "0            0.0      0.0       0.0            0.0           0.0   \n",
      "1            0.0      0.0       0.0            0.0           0.0   \n",
      "2            0.0      0.0       0.0            0.0           0.0   \n",
      "3            0.0      0.0       0.0            0.0           0.0   \n",
      "4            0.0      0.0       0.0            0.0           0.0   \n",
      "\n",
      "   Attack_label  Attack_type  \n",
      "0             1         MITM  \n",
      "1             1         MITM  \n",
      "2             1         MITM  \n",
      "3             1         MITM  \n",
      "4             1         MITM  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "\n",
      "Missing values:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nDimensions: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nMissing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        print(missing[missing > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49079a",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "**Remove duplicates, handle missing values, and convert data types.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112e5d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DNN-EdgeIIoT-dataset...\n",
      "  Removed 815 duplicate rows\n",
      "  Removed 815 duplicate rows\n",
      "  Converted 19 object columns to numeric\n",
      "  Final: 2,218,386 rows (100.0% retained)\n",
      "\n",
      "Cleaning live_data_training...\n",
      "  Converted 19 object columns to numeric\n",
      "  Final: 2,218,386 rows (100.0% retained)\n",
      "\n",
      "Cleaning live_data_training...\n",
      "  Removed 77 duplicate rows\n",
      "  Removed 77 duplicate rows\n",
      "  Converted 6 object columns to numeric\n",
      "  Final: 32,404 rows (99.8% retained)\n",
      "\n",
      "Cleaning ML-EdgeIIoT-dataset...\n",
      "  Converted 6 object columns to numeric\n",
      "  Final: 32,404 rows (99.8% retained)\n",
      "\n",
      "Cleaning ML-EdgeIIoT-dataset...\n",
      "  Removed 814 duplicate rows\n",
      "  Removed 814 duplicate rows\n",
      "  Converted 19 object columns to numeric\n",
      "  Final: 156,986 rows (99.5% retained)\n",
      "\n",
      "  Converted 19 object columns to numeric\n",
      "  Final: 156,986 rows (99.5% retained)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df, name):\n",
    "    \"\"\"Clean and validate dataset\"\"\"\n",
    "    print(f\"Cleaning {name}...\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df.drop_duplicates().reset_index(drop=True)\n",
    "    duplicates = initial_rows - len(df_clean)\n",
    "    if duplicates > 0:\n",
    "        print(f\"  Removed {duplicates} duplicate rows\")\n",
    "    \n",
    "    # Drop rows with any missing values\n",
    "    rows_before = len(df_clean)\n",
    "    df_clean = df_clean.dropna()\n",
    "    missing_removed = rows_before - len(df_clean)\n",
    "    if missing_removed > 0:\n",
    "        print(f\"  Removed {missing_removed} rows with missing values\")\n",
    "    \n",
    "    # Convert object columns to numeric where possible\n",
    "    converted_success = 0\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            try:\n",
    "                converted = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                if converted.notna().sum() > 0:\n",
    "                    df_clean[col] = converted\n",
    "                    converted_success += 1\n",
    "            except:\n",
    "                pass\n",
    "    if converted_success > 0:\n",
    "        print(f\"  Converted {converted_success} object columns to numeric\")\n",
    "    \n",
    "    # Add source dataset indicator\n",
    "    df_clean['dataset_source'] = name\n",
    "    \n",
    "    print(f\"  Final: {len(df_clean):,} rows ({100*len(df_clean)/initial_rows:.1f}% retained)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "cleaned_datasets = {}\n",
    "for name, df in datasets.items():\n",
    "    cleaned_datasets[name] = clean_dataset(df, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03418bb",
   "metadata": {},
   "source": [
    "## 5. Merge Datasets\n",
    "\n",
    "Combine all cleaned datasets into a single dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72363fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNN-EdgeIIoT-dataset'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cleaned_datasets.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f16ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets...\n",
      "Total unique columns across all datasets: 64\n",
      "Columns in all datasets: 64\n",
      "Common columns: ['Attack_label', 'Attack_type', 'arp.dst.proto_ipv4', 'arp.hw.size', 'arp.opcode', 'arp.src.proto_ipv4', 'dataset_source', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in', 'frame.time', 'http.content_length', 'http.file_data', 'http.referer', 'http.request.full_uri', 'http.request.method', 'http.request.uri.query', 'http.request.version', 'http.response', 'http.tls_port', 'icmp.checksum', 'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused', 'ip.dst_host', 'ip.src_host', 'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id', 'mqtt.conack.flags', 'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg', 'mqtt.msg_decoded_as', 'mqtt.msgtype', 'mqtt.proto_len', 'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver', 'tcp.ack', 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options', 'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream', 'udp.time_delta']\n",
      "\n",
      "Merged dataset shape: (2407776, 64)\n",
      "Total rows: 2,407,776\n",
      "Total columns: 64\n",
      "\n",
      "Dataset sources distribution:\n",
      "dataset_source\n",
      "DNN-EdgeIIoT-dataset    2218386\n",
      "ML-EdgeIIoT-dataset      156986\n",
      "live_data_training        32404\n",
      "Name: count, dtype: int64\n",
      "Merged dataset shape: (2407776, 64)\n",
      "Total rows: 2,407,776\n",
      "Total columns: 64\n",
      "\n",
      "Dataset sources distribution:\n",
      "dataset_source\n",
      "DNN-EdgeIIoT-dataset    2218386\n",
      "ML-EdgeIIoT-dataset      156986\n",
      "live_data_training        32404\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data quality metrics:\n",
      "  Total cells: 154,097,664\n",
      "  Non-null cells: 145,987,983\n",
      "  Sparsity: 5.3%\n",
      "\n",
      "Missing values per column (top 10):\n",
      "\n",
      "Data quality metrics:\n",
      "  Total cells: 154,097,664\n",
      "  Non-null cells: 145,987,983\n",
      "  Sparsity: 5.3%\n",
      "\n",
      "Missing values per column (top 10):\n",
      "frame.time           2374572\n",
      "ip.src_host          2082815\n",
      "ip.dst_host          1959673\n",
      "tcp.payload           603615\n",
      "tcp.options           532180\n",
      "mqtt.conack.flags      84306\n",
      "mqtt.protoname         84285\n",
      "mqtt.topic             84262\n",
      "mqtt.msg               84262\n",
      "http.file_data         53869\n",
      "dtype: int64\n",
      "frame.time           2374572\n",
      "ip.src_host          2082815\n",
      "ip.dst_host          1959673\n",
      "tcp.payload           603615\n",
      "tcp.options           532180\n",
      "mqtt.conack.flags      84306\n",
      "mqtt.protoname         84285\n",
      "mqtt.topic             84262\n",
      "mqtt.msg               84262\n",
      "http.file_data         53869\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging datasets...\")\n",
    "\n",
    "# Analyze columns across datasets\n",
    "all_columns = set()\n",
    "for df in cleaned_datasets.values():\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "print(f\"Total unique columns across all datasets: {len(all_columns)}\")\n",
    "\n",
    "# Find common columns\n",
    "common_cols = set(cleaned_datasets[list(cleaned_datasets.keys())[0]].columns)\n",
    "for df in list(cleaned_datasets.values())[1:]:\n",
    "    common_cols &= set(df.columns)\n",
    "\n",
    "print(f\"Columns in all datasets: {len(common_cols)}\")\n",
    "print(f\"Common columns: {sorted(common_cols)}\\n\")\n",
    "\n",
    "# Merge all datasets\n",
    "df_merged = pd.concat(cleaned_datasets.values(), ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"Merged dataset shape: {df_merged.shape}\")\n",
    "print(f\"Total rows: {len(df_merged):,}\")\n",
    "print(f\"Total columns: {len(df_merged.columns)}\")\n",
    "\n",
    "print(f\"\\nDataset sources distribution:\")\n",
    "print(df_merged['dataset_source'].value_counts())\n",
    "\n",
    "# Calculate data sparsity\n",
    "total_cells = df_merged.shape[0] * df_merged.shape[1]\n",
    "non_null = df_merged.notna().sum().sum()\n",
    "sparsity = (1 - non_null / total_cells) * 100\n",
    "\n",
    "print(f\"\\nData quality metrics:\")\n",
    "print(f\"  Total cells: {total_cells:,}\")\n",
    "print(f\"  Non-null cells: {non_null:,}\")\n",
    "print(f\"  Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "print(f\"\\nMissing values per column (top 10):\")\n",
    "missing_per_col = df_merged.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_per_col.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0089a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for device identifier columns...\n",
      "No device identifier column found\n",
      "Using dataset source + random grouping instead\n",
      "\n",
      "Created 2407 synthetic device IDs per dataset\n",
      "\n",
      "Devices distribution:\n",
      "device_id\n",
      "0       1002\n",
      "1       1002\n",
      "2       1002\n",
      "3       1002\n",
      "4       1002\n",
      "        ... \n",
      "2402     999\n",
      "2403     999\n",
      "2404     999\n",
      "2405     999\n",
      "2406     999\n",
      "Name: count, Length: 2407, dtype: int64\n",
      "\n",
      "Device counts by dataset:\n",
      "Created 2407 synthetic device IDs per dataset\n",
      "\n",
      "Devices distribution:\n",
      "device_id\n",
      "0       1002\n",
      "1       1002\n",
      "2       1002\n",
      "3       1002\n",
      "4       1002\n",
      "        ... \n",
      "2402     999\n",
      "2403     999\n",
      "2404     999\n",
      "2405     999\n",
      "2406     999\n",
      "Name: count, Length: 2407, dtype: int64\n",
      "\n",
      "Device counts by dataset:\n",
      "device_id             0     1     2     3     4     5     6     7     8     \\\n",
      "dataset_source                                                               \n",
      "DNN-EdgeIIoT-dataset   922   922   922   922   922   922   922   922   922   \n",
      "ML-EdgeIIoT-dataset     66    66    66    66    66    66    66    66    66   \n",
      "live_data_training      14    14    14    14    14    14    14    14    14   \n",
      "\n",
      "device_id             9     ...  2397  2398  2399  2400  2401  2402  2403  \\\n",
      "dataset_source              ...                                             \n",
      "DNN-EdgeIIoT-dataset   922  ...   921   921   921   921   921   921   921   \n",
      "ML-EdgeIIoT-dataset     66  ...    65    65    65    65    65    65    65   \n",
      "live_data_training      14  ...    13    13    13    13    13    13    13   \n",
      "\n",
      "device_id             2404  2405  2406  \n",
      "dataset_source                          \n",
      "DNN-EdgeIIoT-dataset   921   921   921  \n",
      "ML-EdgeIIoT-dataset     65    65    65  \n",
      "live_data_training      13    13    13  \n",
      "\n",
      "[3 rows x 2407 columns]\n",
      "device_id             0     1     2     3     4     5     6     7     8     \\\n",
      "dataset_source                                                               \n",
      "DNN-EdgeIIoT-dataset   922   922   922   922   922   922   922   922   922   \n",
      "ML-EdgeIIoT-dataset     66    66    66    66    66    66    66    66    66   \n",
      "live_data_training      14    14    14    14    14    14    14    14    14   \n",
      "\n",
      "device_id             9     ...  2397  2398  2399  2400  2401  2402  2403  \\\n",
      "dataset_source              ...                                             \n",
      "DNN-EdgeIIoT-dataset   922  ...   921   921   921   921   921   921   921   \n",
      "ML-EdgeIIoT-dataset     66  ...    65    65    65    65    65    65    65   \n",
      "live_data_training      14  ...    13    13    13    13    13    13    13   \n",
      "\n",
      "device_id             2404  2405  2406  \n",
      "dataset_source                          \n",
      "DNN-EdgeIIoT-dataset   921   921   921  \n",
      "ML-EdgeIIoT-dataset     65    65    65  \n",
      "live_data_training      13    13    13  \n",
      "\n",
      "[3 rows x 2407 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for existing device/identifier columns\n",
    "print(\"Looking for device identifier columns...\")\n",
    "device_col_candidates = ['device_id', 'Device_ID', 'DeviceID', 'device', 'Device', 'id', 'ID', 'src_ip', 'dst_ip', 'ip.src', 'ip.dst']\n",
    "\n",
    "device_col = None\n",
    "for col in device_col_candidates:\n",
    "    if col in df_merged.columns:\n",
    "        # Check if column has meaningful values (not all NaN)\n",
    "        if df_merged[col].notna().sum() > 0:\n",
    "            device_col = col\n",
    "            print(f\"Found column: {col}\")\n",
    "            break\n",
    "\n",
    "if device_col is None:\n",
    "    print(\"No device identifier column found\")\n",
    "    print(f\"Using dataset source + random grouping instead\\n\")\n",
    "    \n",
    "    # Group by dataset source and create device IDs within each\n",
    "    num_devices = max(5, len(df_merged) // 1000)\n",
    "    df_merged['device_id'] = df_merged.groupby('dataset_source').cumcount() % num_devices\n",
    "    print(f\"Created {num_devices} synthetic device IDs per dataset\")\n",
    "else:\n",
    "    print(f\"Using existing device column: {device_col}\")\n",
    "    df_merged.rename(columns={device_col: 'device_id'}, inplace=True)\n",
    "\n",
    "print(f\"\\nDevices distribution:\")\n",
    "print(df_merged['device_id'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDevice counts by dataset:\")\n",
    "print(df_merged.groupby(['dataset_source', 'device_id']).size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806eaf8",
   "metadata": {},
   "source": [
    "## 6. Group by Device\n",
    "\n",
    "Organize the dataset by device for streaming preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc81e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping data by device...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 460. KiB for an array with shape (59, 999) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m device_groups \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device_id, group \u001b[38;5;129;01min\u001b[39;00m df_merged\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     device_groups[\u001b[38;5;28mstr\u001b[39m(device_id)] \u001b[38;5;241m=\u001b[39m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(device_groups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m device groups\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice group statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:6154\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[0;32m   6152\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   6153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6154\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_duplicates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   6156\u001b[0m     allow_duplicates \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(allow_duplicates, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:664\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    661\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 664\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2270\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2275\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\imadb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2303\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2304\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2305\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2307\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 460. KiB for an array with shape (59, 999) and data type float64"
     ]
    }
   ],
   "source": [
    "print(\"Grouping data by device...\")\n",
    "\n",
    "device_groups = {}\n",
    "for device_id, group in df_merged.groupby('device_id'):\n",
    "    device_groups[str(device_id)] = group.reset_index(drop=True)\n",
    "\n",
    "print(f\"Created {len(device_groups)} device groups\\n\")\n",
    "\n",
    "print(\"Device group statistics:\")\n",
    "print(\"-\" * 60)\n",
    "for device_id, group in sorted(device_groups.items()):\n",
    "    print(f\"Device {device_id}: {len(group)} rows\")\n",
    "\n",
    "print(f\"\\nData grouped by device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49e865",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save preprocessed data and statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bd04b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./edge_iiot_processed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m output_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExporting processed data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = Path('./edge_iiot_processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Exporting processed data to {output_dir}\\n\")\n",
    "\n",
    "# Export merged dataset\n",
    "merged_file = output_dir / 'merged_data.csv'\n",
    "df_merged.to_csv(merged_file, index=False)\n",
    "print(f\"Merged data: {merged_file}\")\n",
    "print(f\"  Size: {merged_file.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# Export device-specific files\n",
    "print(f\"\\nDevice files:\")\n",
    "for device_id, df_device in device_groups.items():\n",
    "    device_file = output_dir / f'device_{device_id}.csv'\n",
    "    df_device.to_csv(device_file, index=False)\n",
    "    print(f\"  device_{device_id}.csv ({len(df_device)} rows)\")\n",
    "\n",
    "# Export summary\n",
    "summary = {\n",
    "    'total_records': len(df_merged),\n",
    "    'total_devices': len(device_groups),\n",
    "    'total_columns': len(df_merged.columns),\n",
    "    'dataset_sources': df_merged['dataset_source'].value_counts().to_dict(),\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "summary_file = output_dir / 'processing_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nExport complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46627906",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed the following steps:\n",
    "\n",
    "1. Downloaded the Edge-IIoT dataset from Kaggle\n",
    "2. Loaded and analyzed three CSV files\n",
    "3. Cleaned data by removing duplicates and handling missing values\n",
    "4. Merged datasets into a consolidated dataframe\n",
    "5. Organized data by device identifier\n",
    "6. Converted data to JSON message format suitable for streaming\n",
    "7. Demonstrated streaming at 10 rows per second\n",
    "8. Exported processed data to multiple formats\n",
    "\n",
    "### Output files\n",
    "- `merged_data.csv` - Complete merged dataset\n",
    "- `device_*.csv` - Per-device data files  \n",
    "- `kafka_queue.json` - Formatted for Kafka streaming\n",
    "- `processing_summary.json` - Processing statistics\n",
    "\n",
    "The preprocessed data is ready for integration with a Kafka broker or other streaming systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
