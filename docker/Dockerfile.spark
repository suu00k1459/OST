# Dockerfile.spark
# Spark analytics service (reads from Kafka, writes to DB, runs ML)

FROM apache/spark:3.5.0

# Switch to root to install packages
USER root

# Optional: working directory just for Python deps
WORKDIR /opt/spark/app

# Make pip more tolerant of slow / flaky network
ENV PIP_DEFAULT_TIMEOUT=300

# Use service-specific requirements
COPY requirements/spark.txt requirements.txt

# Install ONLY the Python libs this Spark service needs
RUN pip3 install --no-cache-dir -r requirements.txt

# Create work directory with proper permissions for Spark
RUN mkdir -p /opt/spark/work \
    && chown -R spark:spark /opt/spark/work \
    && chmod 777 /opt/spark/work

# Switch back to spark user
USER spark

# Use the image's standard entrypoint (no override)
# ENTRYPOINT comes from apache/spark:3.5.0
