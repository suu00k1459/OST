# Dockerfile.spark
# Spark analytics service (reads from Kafka, writes to DB, runs ML)

FROM apache/spark:3.5.0

# Switch to root to install packages
USER root

# Optional: working directory just for Python deps
WORKDIR /opt/spark/app

# Make pip more tolerant of slow / flaky network
ENV PIP_DEFAULT_TIMEOUT=300

# Use service-specific requirements
COPY requirements/spark.txt requirements.txt

# Install ONLY the Python libs this Spark service needs
RUN pip3 install --no-cache-dir -r requirements.txt

# Create work directory with proper permissions for Spark
# Also create Ivy cache for Maven dependencies download
RUN mkdir -p /opt/spark/work \
    && mkdir -p /home/spark/.ivy2/cache /home/spark/.ivy2/jars \
    && chown -R spark:spark /opt/spark/work /home/spark/.ivy2 \
    && chmod 777 /opt/spark/work

# Pre-download Kafka connector dependencies at build time
# This prevents runtime download failures and speeds up job submission
RUN /opt/spark/bin/spark-submit \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
    --class org.apache.spark.examples.SparkPi \
    --master local[1] \
    /opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar 1 2>/dev/null || true \
    && cp -r /root/.ivy2/* /home/spark/.ivy2/ 2>/dev/null || true \
    && chown -R spark:spark /home/spark/.ivy2

# Switch back to spark user
USER spark

# Use the image's standard entrypoint (no override)
# ENTRYPOINT comes from apache/spark:3.5.0
